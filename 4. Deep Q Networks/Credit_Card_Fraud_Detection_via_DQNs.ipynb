{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit_Card_Fraud_Detection_via_DQNs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOurItW2+dVCTiu/Hs2wf3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-Reinforcement-Learning/blob/master/4.%20Deep%20Q%20Networks/Credit_Card_Fraud_Detection_via_DQNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd3nIxkWx9re",
        "colab_type": "text"
      },
      "source": [
        "# Downloading data and Making *gym-fraud-detection* environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEfJsOzcr5A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/gym-fraud-detection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSP-HmORxLFf",
        "colab_type": "code",
        "outputId": "622ad7fb-73d7-4a31-aad0-2c3f158346b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "!unzip dataset.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/creditcard.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJDVKS1Zx5Vu",
        "colab_type": "code",
        "outputId": "88795ce3-4440-43f2-9887-08b146df251d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbw-KPaCybIS",
        "colab_type": "code",
        "outputId": "e658075e-68b6-4948-bff4-210a19013f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!unzip gym-fraud-detection.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  gym-fraud-detection.zip\n",
            "   creating: gym-fraud-detection/\n",
            "  inflating: gym-fraud-detection/setup.py  \n",
            "  inflating: gym-fraud-detection/README.md  \n",
            "   creating: gym-fraud-detection/gym_fraud_detection/\n",
            "  inflating: gym-fraud-detection/gym_fraud_detection/__init__.py  \n",
            "   creating: gym-fraud-detection/.idea/\n",
            "  inflating: gym-fraud-detection/.idea/workspace.xml  \n",
            "  inflating: gym-fraud-detection/.idea/vcs.xml  \n",
            "  inflating: gym-fraud-detection/.idea/gym-fraud-detection.iml  \n",
            "  inflating: gym-fraud-detection/.idea/modules.xml  \n",
            "  inflating: gym-fraud-detection/.idea/encodings.xml  \n",
            "  inflating: gym-fraud-detection/.idea/misc.xml  \n",
            "   creating: gym-fraud-detection/.git/\n",
            "  inflating: gym-fraud-detection/.git/index  \n",
            " extracting: gym-fraud-detection/.git/ORIG_HEAD  \n",
            "  inflating: gym-fraud-detection/.git/FETCH_HEAD  \n",
            " extracting: gym-fraud-detection/.git/COMMIT_EDITMSG  \n",
            " extracting: gym-fraud-detection/.git/HEAD  \n",
            "  inflating: gym-fraud-detection/.git/packed-refs  \n",
            "  inflating: gym-fraud-detection/.git/config  \n",
            "  inflating: gym-fraud-detection/.git/description  \n",
            "   creating: gym-fraud-detection/gym_fraud_detection/envs/\n",
            "  inflating: gym-fraud-detection/gym_fraud_detection/envs/test.py  \n",
            "  inflating: gym-fraud-detection/gym_fraud_detection/envs/__init__.py  \n",
            "  inflating: gym-fraud-detection/gym_fraud_detection/envs/fraud_detection_env.py  \n",
            "   creating: gym-fraud-detection/.git/logs/\n",
            "  inflating: gym-fraud-detection/.git/logs/HEAD  \n",
            "   creating: gym-fraud-detection/.git/objects/\n",
            "   creating: gym-fraud-detection/.git/info/\n",
            "  inflating: gym-fraud-detection/.git/info/exclude  \n",
            "   creating: gym-fraud-detection/.git/branches/\n",
            "   creating: gym-fraud-detection/.git/hooks/\n",
            "  inflating: gym-fraud-detection/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/pre-push.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/pre-commit.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/update.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/post-update.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/commit-msg.sample  \n",
            "  inflating: gym-fraud-detection/.git/hooks/pre-rebase.sample  \n",
            "   creating: gym-fraud-detection/.git/refs/\n",
            "   creating: gym-fraud-detection/.git/logs/refs/\n",
            "   creating: gym-fraud-detection/.git/objects/32/\n",
            " extracting: gym-fraud-detection/.git/objects/32/4c4ba385b7cc282d36e64d39ac505258f6e3f3  \n",
            "   creating: gym-fraud-detection/.git/objects/78/\n",
            " extracting: gym-fraud-detection/.git/objects/78/85619782426a7b326a97c83d890964c66c73bb  \n",
            "   creating: gym-fraud-detection/.git/objects/08/\n",
            " extracting: gym-fraud-detection/.git/objects/08/cf6089bd2a28bdcc2abac1f122a74589822719  \n",
            "   creating: gym-fraud-detection/.git/objects/07/\n",
            " extracting: gym-fraud-detection/.git/objects/07/4f816ac68d503058ff6b3597af2e9044c5fa7e  \n",
            " extracting: gym-fraud-detection/.git/objects/07/f6be5ca19da9b3947e0449896ba7903c5ea048  \n",
            "   creating: gym-fraud-detection/.git/objects/fc/\n",
            " extracting: gym-fraud-detection/.git/objects/fc/9e781039da8508b6f13b894d538ca9b9a1c6c8  \n",
            "   creating: gym-fraud-detection/.git/objects/eb/\n",
            " extracting: gym-fraud-detection/.git/objects/eb/91aa64e9d6ca3485acaa05ce92142e652bb308  \n",
            "   creating: gym-fraud-detection/.git/objects/f3/\n",
            " extracting: gym-fraud-detection/.git/objects/f3/245f0b8f42f0aba80fc0ba5eca15a3baf19a90  \n",
            "   creating: gym-fraud-detection/.git/objects/41/\n",
            " extracting: gym-fraud-detection/.git/objects/41/d3458a5854739ca176fb49d4f5dd88b6db9a80  \n",
            "   creating: gym-fraud-detection/.git/objects/16/\n",
            " extracting: gym-fraud-detection/.git/objects/16/38132f33ce2093940bcc49bda144057f126484  \n",
            "   creating: gym-fraud-detection/.git/objects/ef/\n",
            " extracting: gym-fraud-detection/.git/objects/ef/874fc31a69252120e4d876237956456bdb7bd4  \n",
            "   creating: gym-fraud-detection/.git/objects/57/\n",
            " extracting: gym-fraud-detection/.git/objects/57/1d4f955df1c5f2866d5b1c2e57defa3e3a9cd6  \n",
            "   creating: gym-fraud-detection/.git/objects/29/\n",
            " extracting: gym-fraud-detection/.git/objects/29/7fffa2396b3db6ac41fd129f5a7d2ea0b58da0  \n",
            "   creating: gym-fraud-detection/.git/objects/58/\n",
            " extracting: gym-fraud-detection/.git/objects/58/2013069d105641a1b4420a358c0fb5a26f5f8d  \n",
            "   creating: gym-fraud-detection/.git/objects/ca/\n",
            " extracting: gym-fraud-detection/.git/objects/ca/29118949266f78335170d12f62bc604a7c209a  \n",
            "   creating: gym-fraud-detection/.git/objects/8d/\n",
            " extracting: gym-fraud-detection/.git/objects/8d/3ebec36c5ccafa5edae61fd82250de16e0b364  \n",
            "   creating: gym-fraud-detection/.git/objects/09/\n",
            " extracting: gym-fraud-detection/.git/objects/09/ca86caa7dc28877843fecea57e2eec9ffaabf9  \n",
            "   creating: gym-fraud-detection/.git/objects/79/\n",
            " extracting: gym-fraud-detection/.git/objects/79/9da6a34ae67f61f45ac2f570bd7efa6b1d9d7a  \n",
            "   creating: gym-fraud-detection/.git/objects/f6/\n",
            " extracting: gym-fraud-detection/.git/objects/f6/d0d9960a2ea2e0173a10193a47254649da5da0  \n",
            "   creating: gym-fraud-detection/.git/objects/7f/\n",
            " extracting: gym-fraud-detection/.git/objects/7f/ef9edc6cbedd3e126c8e14789fca5b8342fdb6  \n",
            "   creating: gym-fraud-detection/.git/objects/10/\n",
            " extracting: gym-fraud-detection/.git/objects/10/673a85733ae662e2d15675e1c8868d10ba3339  \n",
            "   creating: gym-fraud-detection/.git/objects/64/\n",
            " extracting: gym-fraud-detection/.git/objects/64/8872169adeb60600106fd5471ce2ad97abe5a1  \n",
            "   creating: gym-fraud-detection/.git/objects/a2/\n",
            " extracting: gym-fraud-detection/.git/objects/a2/d7823c583d2b39c423153d933a24b5ad8953f0  \n",
            "   creating: gym-fraud-detection/.git/objects/b5/\n",
            " extracting: gym-fraud-detection/.git/objects/b5/40e79c7af20c5bbf0e4f5d925e313b4dbf1c7a  \n",
            "   creating: gym-fraud-detection/.git/objects/b8/\n",
            " extracting: gym-fraud-detection/.git/objects/b8/2dc7b54504e0467bccc5b103cf655f47e9a5e3  \n",
            "   creating: gym-fraud-detection/.git/objects/3d/\n",
            " extracting: gym-fraud-detection/.git/objects/3d/88519c43e666b55879997a017a7c3bd5a8fd14  \n",
            "   creating: gym-fraud-detection/.git/objects/88/\n",
            " extracting: gym-fraud-detection/.git/objects/88/02f8f83e9fa47e7eebdb98abc8ca6ae62433df  \n",
            "   creating: gym-fraud-detection/.git/objects/80/\n",
            " extracting: gym-fraud-detection/.git/objects/80/9b951a829300318f8a28f96c9b9c1060cb69f5  \n",
            "   creating: gym-fraud-detection/.git/objects/f1/\n",
            " extracting: gym-fraud-detection/.git/objects/f1/d5fd6850511f9ac6c6e33bbda28730d7ffee88  \n",
            " extracting: gym-fraud-detection/.git/objects/f1/d668439da4348216f4114dfacabfab6c69607b  \n",
            "   creating: gym-fraud-detection/.git/objects/28/\n",
            " extracting: gym-fraud-detection/.git/objects/28/c41eec66e23aeb46a3d2148a61cf7ff73aff7c  \n",
            "   creating: gym-fraud-detection/.git/objects/e6/\n",
            " extracting: gym-fraud-detection/.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391  \n",
            "   creating: gym-fraud-detection/.git/objects/94/\n",
            " extracting: gym-fraud-detection/.git/objects/94/2737a0fd356bb3585794c8a9fbe2f9aceec1f4  \n",
            " extracting: gym-fraud-detection/.git/objects/94/a25f7f4cb416c083d265558da75d457237d671  \n",
            "   creating: gym-fraud-detection/.git/objects/ea/\n",
            " extracting: gym-fraud-detection/.git/objects/ea/0f4b993d850a959169ada12626e4277ac38e2b  \n",
            "   creating: gym-fraud-detection/.git/objects/25/\n",
            " extracting: gym-fraud-detection/.git/objects/25/7e24b080ac31ddcf1dc4b97dd9529a42d83274  \n",
            "   creating: gym-fraud-detection/.git/objects/84/\n",
            " extracting: gym-fraud-detection/.git/objects/84/423929b1578716a5d107e2ec1b37b137c266f2  \n",
            "   creating: gym-fraud-detection/.git/objects/info/\n",
            "   creating: gym-fraud-detection/.git/objects/pack/\n",
            "   creating: gym-fraud-detection/.git/refs/remotes/\n",
            "   creating: gym-fraud-detection/.git/refs/tags/\n",
            "   creating: gym-fraud-detection/.git/refs/heads/\n",
            " extracting: gym-fraud-detection/.git/refs/heads/master  \n",
            "   creating: gym-fraud-detection/.git/logs/refs/heads/\n",
            "  inflating: gym-fraud-detection/.git/logs/refs/heads/master  \n",
            "   creating: gym-fraud-detection/.git/logs/refs/remotes/\n",
            "   creating: gym-fraud-detection/.git/refs/remotes/origin/\n",
            " extracting: gym-fraud-detection/.git/refs/remotes/origin/master  \n",
            " extracting: gym-fraud-detection/.git/refs/remotes/origin/HEAD  \n",
            "   creating: gym-fraud-detection/.git/logs/refs/remotes/origin/\n",
            "  inflating: gym-fraud-detection/.git/logs/refs/remotes/origin/master  \n",
            "  inflating: gym-fraud-detection/.git/logs/refs/remotes/origin/HEAD  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etBbJvzDyfUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('gym-fraud-detection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki_yImcr00iq",
        "colab_type": "code",
        "outputId": "11a9d0cd-fd4d-496f-e788-bc94d44c32d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/gym-fraud-detection\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-fraud-detection==0.0.1) (0.17.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fraud-detection==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-fraud-detection==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fraud-detection==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fraud-detection==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-fraud-detection==0.0.1) (0.16.0)\n",
            "Installing collected packages: gym-fraud-detection\n",
            "  Running setup.py develop for gym-fraud-detection\n",
            "Successfully installed gym-fraud-detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZn7ZY3S09w4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import gym_fraud_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGDw0SDL65J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content')\n",
        "env = gym.make('gym-fraud-detection-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0oVq2LW7Abr",
        "colab_type": "code",
        "outputId": "7a9f65e5-9b3e-4ef5-dc56-08af8a91147a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(2)\n",
            "Discrete(284807)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyliTenYyJFn",
        "colab_type": "text"
      },
      "source": [
        "# Download proper libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npIKlpFYx7aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxvMcLNyyazd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M2zDvSnzrhP",
        "colab_type": "text"
      },
      "source": [
        "# Exploring the custom gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lBO9cfLyeGI",
        "colab_type": "code",
        "outputId": "18406c1f-3736-41aa-8bb3-84156345982c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(env.action_space.n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkFHCcn2zwsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actions = []\n",
        "rewards = []\n",
        "count = 5\n",
        "\n",
        "while True:\n",
        "  action = env.action_space.sample()\n",
        "  reward = env.step(action)\n",
        "  actions.append(action)\n",
        "  rewards.append(reward)\n",
        "  count -= 1\n",
        "  if count==0:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSpdBwmD0MXf",
        "colab_type": "code",
        "outputId": "391ef567-d341-40e7-965c-8691b8a0dd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(\"action : \", actions[i], \"labelled data : \", env.label_for(i))\n",
        "  print(\"reward : \", rewards[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action :  1 labelled data :  0.0\n",
            "reward :  (array([ 0.        ,  1.19185711,  0.26615071,  0.16648011,  0.44815408,\n",
            "        0.06001765, -0.08236081, -0.07880298,  0.08510165, -0.25542513,\n",
            "       -0.16697441,  1.61272666,  1.06523531,  0.48909502, -0.1437723 ,\n",
            "        0.63555809,  0.46391704, -0.11480466, -0.18336127, -0.14578304,\n",
            "       -0.06908314, -0.22577525, -0.63867195,  0.10128802, -0.33984648,\n",
            "        0.1671704 ,  0.12589453, -0.0089831 ,  0.01472417,  2.69      ]), -1, False, '{\"true_positive_rate\": 0, \"false_positive_rate\": 1, \"true_negative_rate\": 0, \"false_negative_rate\": 0}')\n",
            "action :  1 labelled data :  0.0\n",
            "reward :  (array([ 1.00000000e+00, -1.35835406e+00, -1.34016307e+00,  1.77320934e+00,\n",
            "        3.79779593e-01, -5.03198133e-01,  1.80049938e+00,  7.91460956e-01,\n",
            "        2.47675787e-01, -1.51465432e+00,  2.07642865e-01,  6.24501459e-01,\n",
            "        6.60836853e-02,  7.17292731e-01, -1.65945923e-01,  2.34586495e+00,\n",
            "       -2.89008319e+00,  1.10996938e+00, -1.21359313e-01, -2.26185710e+00,\n",
            "        5.24979725e-01,  2.47998153e-01,  7.71679402e-01,  9.09412262e-01,\n",
            "       -6.89280956e-01, -3.27641834e-01, -1.39096572e-01, -5.53527940e-02,\n",
            "       -5.97518406e-02,  3.78660000e+02]), -1, False, '{\"true_positive_rate\": 0, \"false_positive_rate\": 2, \"true_negative_rate\": 0, \"false_negative_rate\": 0}')\n",
            "action :  0 labelled data :  0.0\n",
            "reward :  (array([ 1.00000000e+00, -9.66271712e-01, -1.85226008e-01,  1.79299334e+00,\n",
            "       -8.63291275e-01, -1.03088796e-02,  1.24720317e+00,  2.37608940e-01,\n",
            "        3.77435875e-01, -1.38702406e+00, -5.49519225e-02, -2.26487264e-01,\n",
            "        1.78228226e-01,  5.07756870e-01, -2.87923745e-01, -6.31418118e-01,\n",
            "       -1.05964725e+00, -6.84092786e-01,  1.96577500e+00, -1.23262197e+00,\n",
            "       -2.08037781e-01, -1.08300452e-01,  5.27359678e-03, -1.90320519e-01,\n",
            "       -1.17557533e+00,  6.47376035e-01, -2.21928844e-01,  6.27228487e-02,\n",
            "        6.14576285e-02,  1.23500000e+02]), 1, False, '{\"true_positive_rate\": 0, \"false_positive_rate\": 2, \"true_negative_rate\": 0, \"false_negative_rate\": 0}')\n",
            "action :  1 labelled data :  0.0\n",
            "reward :  (array([ 2.00000000e+00, -1.15823309e+00,  8.77736755e-01,  1.54871785e+00,\n",
            "        4.03033934e-01, -4.07193377e-01,  9.59214625e-02,  5.92940745e-01,\n",
            "       -2.70532677e-01,  8.17739308e-01,  7.53074432e-01, -8.22842878e-01,\n",
            "        5.38195550e-01,  1.34585159e+00, -1.11966983e+00,  1.75121130e-01,\n",
            "       -4.51449183e-01, -2.37033239e-01, -3.81947870e-02,  8.03486925e-01,\n",
            "        4.08542360e-01, -9.43069713e-03,  7.98278495e-01, -1.37458080e-01,\n",
            "        1.41266984e-01, -2.06009588e-01,  5.02292224e-01,  2.19422230e-01,\n",
            "        2.15153147e-01,  6.99900000e+01]), -1, False, '{\"true_positive_rate\": 0, \"false_positive_rate\": 3, \"true_negative_rate\": 0, \"false_negative_rate\": 0}')\n",
            "action :  0 labelled data :  0.0\n",
            "reward :  (array([ 2.        , -0.42596588,  0.96052304,  1.14110934, -0.16825208,\n",
            "        0.42098688, -0.02972755,  0.47620095,  0.26031433, -0.56867138,\n",
            "       -0.3714072 ,  1.34126198,  0.35989384, -0.35809065, -0.1371337 ,\n",
            "        0.51761681,  0.4017259 , -0.05813282,  0.06865315, -0.03319379,\n",
            "        0.08496767, -0.20825351, -0.5598248 , -0.02639767, -0.37142658,\n",
            "       -0.23279382,  0.10591478,  0.25384422,  0.08108026,  3.67      ]), 1, False, '{\"true_positive_rate\": 0, \"false_positive_rate\": 3, \"true_negative_rate\": 0, \"false_negative_rate\": 0}')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXOAmP2t4PAk",
        "colab_type": "text"
      },
      "source": [
        "# Exploring data and creating pytorch model\n",
        "\n",
        "link : https://www.kaggle.com/dakshmiglani/pytorch-credit-card-fraud-prediction-99-8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6iifaPH0O3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.utils.data as data_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08OKe2xb4pRl",
        "colab_type": "code",
        "outputId": "589a1ece-ea60-4047-dd60-b5a7d7add4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        }
      },
      "source": [
        "df = pd.read_csv('./dataset/creditcard.csv')\n",
        "df.head(1) # give us a sneek preview of the dataset xD"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.5516</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.99139</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "\n",
              "[1 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dn7LVNZ4wLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:, :-1].values \n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JrGWieB5bzu",
        "colab_type": "code",
        "outputId": "77409934-c443-40ef-f6ef-b6c7fa9f0c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "print(df.loc[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time      0.000000\n",
            "V1        1.191857\n",
            "V2        0.266151\n",
            "V3        0.166480\n",
            "V4        0.448154\n",
            "V5        0.060018\n",
            "V6       -0.082361\n",
            "V7       -0.078803\n",
            "V8        0.085102\n",
            "V9       -0.255425\n",
            "V10      -0.166974\n",
            "V11       1.612727\n",
            "V12       1.065235\n",
            "V13       0.489095\n",
            "V14      -0.143772\n",
            "V15       0.635558\n",
            "V16       0.463917\n",
            "V17      -0.114805\n",
            "V18      -0.183361\n",
            "V19      -0.145783\n",
            "V20      -0.069083\n",
            "V21      -0.225775\n",
            "V22      -0.638672\n",
            "V23       0.101288\n",
            "V24      -0.339846\n",
            "V25       0.167170\n",
            "V26       0.125895\n",
            "V27      -0.008983\n",
            "V28       0.014724\n",
            "Amount    2.690000\n",
            "Class     0.000000\n",
            "Name: 1, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtT8nKAr7xew",
        "colab_type": "text"
      },
      "source": [
        "## 2. Defining the Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrYaaTPH7tpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(30, 16)\n",
        "        self.fc2 = nn.Linear(16, 18)\n",
        "        self.fc3 = nn.Linear(18, 20)\n",
        "        self.fc4 = nn.Linear(20, 24)\n",
        "        self.fc5 = nn.Linear(24, 2)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=0.25)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = torch.sigmoid(self.fc5(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEIjnFYWuHVH",
        "colab_type": "text"
      },
      "source": [
        "# Define DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRKfj_w9jI0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random \n",
        "from collections import namedtuple, deque \n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "BUFFER_SIZE = int(1e5)  #replay buffer size\n",
        "BATCH_SIZE = 1         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "EPSILON = 0.8           # probability of chosing on-policy action\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wtw7yjqujcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, action_size, seed):\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "\n",
        "    # Q - Network\n",
        "    self.qnet_local = DQN().double().to(device)\n",
        "    self.qnet_target = DQN().double().to(device)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=0.001)\n",
        "\n",
        "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "    # learn every 4 timesteps\n",
        "    self.t_step = (self.t_step+1)%64\n",
        "    if self.t_step == 0:\n",
        "      experience = self.memory.sample()\n",
        "      print('Experience sampled from memory : ', experience)\n",
        "      self.learn(experience, GAMMA)\n",
        "\n",
        "\n",
        "  def epsilon_greedy_action(self, state):\n",
        "    state = state.to(device)\n",
        "    self.qnet_local.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.qnet_local(state).max(1)[1]#.view(1, 1)\n",
        "    self.qnet_local.train()\n",
        "\n",
        "    if random.random() < 0.8:\n",
        "      print('Predicted action based on QNetwork : ', action_values)\n",
        "      return action_values.cpu()\n",
        "    else:\n",
        "      random_action = random.choices(np.arange(self.action_size), k=BATCH_SIZE)\n",
        "      print('Chosing  random actions for the batch : ', random_action)\n",
        "      return random_action\n",
        "  \n",
        "  def learn(self, experiences, gamma):\n",
        "    print('Started learning')\n",
        "    states, actions, rewards, next_states, done = experiences#experiences[0].state, experiences[0].action, experiences[0].reward, experiences[0].next_state, experiences[0].done \n",
        "    criterion = torch.nn.BCELoss()\n",
        "    self.qnet_local.train()\n",
        "    self.qnet_target.eval()\n",
        "\n",
        "    #predicted_targets = self.qnet_local(states)#.gather(1, actions)\n",
        "\n",
        "    #print(next_states.view(1, 1))\n",
        "    with torch.no_grad():\n",
        "      labels_next = self.qnet_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    \n",
        "    print('labels_next {}'.format(labels_next))\n",
        "    \n",
        "    labels = rewards + (gamma * labels_next)\n",
        "    predicted_targets = self.qnet_local(states).gather(1, actions.long())\n",
        "\n",
        "    loss = criterion(predicted_targets, labels).to(device)\n",
        "    print(\"===========================Training loss ============================\")\n",
        "    print(loss)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # perform soft update\n",
        "    self.soft_update(self.qnet_local, self.qnet_target, TAU)\n",
        "  \n",
        "  def soft_update(self, local_model, target_model, tau):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmJtKxIW2ekm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=buffer_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.experiences = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    self.seed = random.seed(seed)\n",
        "  \n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    experience = self.experiences(state, action, reward, next_state, done)\n",
        "    self.memory.append(experience)\n",
        "\n",
        "  def sample(self):\n",
        "     experiences = random.sample(self.memory, k=self.batch_size)\n",
        "     print('Experiences : ', experiences)\n",
        "     states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).double().to(device)\n",
        "     actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).double().to(device)\n",
        "     rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).double().to(device)\n",
        "     next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).double().to(device)\n",
        "     dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).double().to(device)\n",
        "     return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    # experiences = random.sample(self.memory, k=BATCH_SIZE)\n",
        "\n",
        "    # batch = self.experiences(*zip(experiences))\n",
        "\n",
        "    # states = torch.cat(batch.state)\n",
        "    # actions = torch.cat(batch.actions)\n",
        "    # rewards = torch.cat(batch.reward)\n",
        "    # next_states = torch.cat(batch.next_state)\n",
        "    # dones = torch.cat(batch.done)\n",
        "    #return random.sample(self.memory, BATCH_SIZE)\n",
        "\n",
        "    \n",
        "  \n",
        "  def __len__(self):\n",
        "      return len(self.memory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zAspyGi7QgB",
        "colab_type": "text"
      },
      "source": [
        "# Training the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxKMYE789WrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
        "\n",
        "X_train = torch.from_numpy(X_train)\n",
        "Y_train = torch.from_numpy(Y_train).double()\n",
        "\n",
        "train = data_utils.TensorDataset(X_train, Y_train)\n",
        "train_loader = data_utils.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xOHIUkg481e",
        "colab_type": "code",
        "outputId": "8055ae83-5759-438f-cf36-56013aa7429d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# check reward strategy once\n",
        "# add probability to epsilon_greedy\n",
        "deep_agent = Agent(action_size=2, seed=0)\n",
        "num_episodes = 200\n",
        "max_t = 1000\n",
        "state = 0\n",
        "\n",
        "current_state = df.iloc[0, :-1].values\n",
        "\n",
        "for i in range(num_episodes):  \n",
        "  print(\"==========================EPOCH {} COMPLETED===================\".format(i))\n",
        "  \n",
        "  #print('Current state : ', current_state)\n",
        "  score = 0\n",
        "  for state_idx, data in enumerate(train_loader, 0):\n",
        "    print(\"state_idx : \", state_idx)\n",
        "    inputs, labels = data    \n",
        "    action = deep_agent.epsilon_greedy_action(inputs)\n",
        "    #print('action taken : ', action)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    #print('Next state dtype ; ', next_state.dtype)\n",
        "    #print(next_state)\n",
        "    deep_agent.step(current_state, action, reward, next_state, done)\n",
        "    current_state = next_state\n",
        "    #state = next_state\n",
        "    #score += reward      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72488\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72489\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72490\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72491\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72492\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72493\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72494\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72495\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72496\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72497\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72498\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72499\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72500\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72501\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72502\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72503\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72504\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72505\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72506\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72507\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72508\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72509\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72510\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72511\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.29820000e+04,  1.10079516e+00, -2.56194548e-01,  9.36156540e-01,\n",
            "        9.07994593e-01, -9.56729027e-01, -3.87046981e-01, -3.79389805e-01,\n",
            "       -2.98329364e-02,  7.63302074e-01, -2.43269664e-01, -7.07413387e-01,\n",
            "        5.79705778e-01,  3.68581297e-01, -3.84264418e-01,  4.31042016e-02,\n",
            "       -2.29123125e-02, -1.31786848e-01, -2.09143310e-01, -8.22493395e-02,\n",
            "        2.07216659e-02,  9.55101735e-02,  4.04238076e-01, -1.53167392e-01,\n",
            "        4.78845373e-01,  4.84401496e-01,  5.03031673e-01, -2.86773882e-03,\n",
            "        2.87745263e-02,  5.99000000e+01]), action=tensor([0]), reward=1, next_state=array([ 3.29830000e+04,  7.52599706e-01, -1.38906728e+00, -1.21106547e-01,\n",
            "       -2.42731716e-01, -9.34600739e-01, -2.06323307e-01,  3.06867749e-02,\n",
            "       -1.37998664e-01, -9.28321015e-01,  3.90564045e-01,  8.66115443e-02,\n",
            "        4.93862860e-01,  5.39841799e-01,  7.38460391e-02,  4.25466387e-01,\n",
            "       -2.46824925e+00,  9.80652559e-01, -4.15499906e-01, -1.40436039e+00,\n",
            "        2.43394741e-02, -1.96232259e-01, -5.05659314e-01, -1.57087219e-01,\n",
            "        1.68666150e-01,  1.53519119e-01,  1.09959975e+00, -8.53125233e-02,\n",
            "        4.43331803e-02,  2.87750000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.2982e+04,  1.1008e+00, -2.5619e-01,  9.3616e-01,  9.0799e-01,\n",
            "         -9.5673e-01, -3.8705e-01, -3.7939e-01, -2.9833e-02,  7.6330e-01,\n",
            "         -2.4327e-01, -7.0741e-01,  5.7971e-01,  3.6858e-01, -3.8426e-01,\n",
            "          4.3104e-02, -2.2912e-02, -1.3179e-01, -2.0914e-01, -8.2249e-02,\n",
            "          2.0722e-02,  9.5510e-02,  4.0424e-01, -1.5317e-01,  4.7885e-01,\n",
            "          4.8440e-01,  5.0303e-01, -2.8677e-03,  2.8775e-02,  5.9900e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.2983e+04,  7.5260e-01, -1.3891e+00, -1.2111e-01, -2.4273e-01,\n",
            "         -9.3460e-01, -2.0632e-01,  3.0687e-02, -1.3800e-01, -9.2832e-01,\n",
            "          3.9056e-01,  8.6612e-02,  4.9386e-01,  5.3984e-01,  7.3846e-02,\n",
            "          4.2547e-01, -2.4682e+00,  9.8065e-01, -4.1550e-01, -1.4044e+00,\n",
            "          2.4339e-02, -1.9623e-01, -5.0566e-01, -1.5709e-01,  1.6867e-01,\n",
            "          1.5352e-01,  1.0996e+00, -8.5313e-02,  4.4333e-02,  2.8775e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72512\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72513\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72514\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72515\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72516\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72517\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72518\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72519\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72520\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72521\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72522\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72523\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72524\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72525\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72526\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72527\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72528\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72529\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72530\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72531\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72532\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72533\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72534\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72535\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72536\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72537\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72538\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72539\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72540\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72541\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72542\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72543\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72544\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72545\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72546\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72547\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72548\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72549\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72550\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72551\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72552\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72553\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72554\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72555\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72556\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72557\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72558\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72559\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72560\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72561\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72562\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72563\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72564\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72565\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72566\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72567\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72568\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72569\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72570\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72571\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72572\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72573\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72574\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72575\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.45570000e+04,  1.22850321e+00, -1.46745320e-01,  2.14797571e-01,\n",
            "       -2.48501950e-01, -4.85144051e-01, -7.26825023e-01, -9.16379609e-02,\n",
            "       -1.16401124e-01,  2.52539828e-01, -2.15192530e-01, -4.64337326e-02,\n",
            "        1.37085055e-01, -9.37113414e-02,  2.82059237e-01,  1.35504642e+00,\n",
            "       -2.87759968e-02,  3.01847547e-02, -9.00357766e-01, -2.90043195e-01,\n",
            "       -5.25827580e-02,  4.00692620e-02,  1.27815904e-01, -1.60716887e-02,\n",
            "        1.84017553e-01,  2.12912118e-01,  1.47886261e+00, -1.02376344e-01,\n",
            "       -4.96039245e-03,  2.49900000e+01]), action=tensor([0]), reward=1, next_state=array([ 3.45580000e+04, -6.96902368e-01, -5.61097948e-01,  2.29675784e+00,\n",
            "       -2.63728620e+00, -6.84012372e-01,  4.30012836e-01, -7.74707340e-01,\n",
            "        4.00567730e-01, -2.50959529e+00,  7.40889160e-01,  1.38730297e+00,\n",
            "       -4.61412793e-01,  1.49155505e-01, -3.53492528e-01,  8.61316264e-02,\n",
            "       -3.92746324e-01,  4.83766064e-01,  1.24992318e-02, -1.36412668e+00,\n",
            "       -3.51678664e-01,  1.02652345e-01,  6.08456650e-01, -2.35426119e-01,\n",
            "       -3.10994447e-01,  1.74095919e-01, -1.44470768e-01,  9.31511210e-02,\n",
            "        5.25376438e-02,  4.40000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.4557e+04,  1.2285e+00, -1.4675e-01,  2.1480e-01, -2.4850e-01,\n",
            "         -4.8514e-01, -7.2683e-01, -9.1638e-02, -1.1640e-01,  2.5254e-01,\n",
            "         -2.1519e-01, -4.6434e-02,  1.3709e-01, -9.3711e-02,  2.8206e-01,\n",
            "          1.3550e+00, -2.8776e-02,  3.0185e-02, -9.0036e-01, -2.9004e-01,\n",
            "         -5.2583e-02,  4.0069e-02,  1.2782e-01, -1.6072e-02,  1.8402e-01,\n",
            "          2.1291e-01,  1.4789e+00, -1.0238e-01, -4.9604e-03,  2.4990e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.4558e+04, -6.9690e-01, -5.6110e-01,  2.2968e+00, -2.6373e+00,\n",
            "         -6.8401e-01,  4.3001e-01, -7.7471e-01,  4.0057e-01, -2.5096e+00,\n",
            "          7.4089e-01,  1.3873e+00, -4.6141e-01,  1.4916e-01, -3.5349e-01,\n",
            "          8.6132e-02, -3.9275e-01,  4.8377e-01,  1.2499e-02, -1.3641e+00,\n",
            "         -3.5168e-01,  1.0265e-01,  6.0846e-01, -2.3543e-01, -3.1099e-01,\n",
            "          1.7410e-01, -1.4447e-01,  9.3151e-02,  5.2538e-02,  4.4000e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72576\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72577\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72578\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72579\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72580\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72581\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72582\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72583\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72584\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72585\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72586\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72587\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72588\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72589\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72590\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72591\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72592\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72593\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72594\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72595\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72596\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72597\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72598\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72599\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72600\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72601\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72602\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72603\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72604\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72605\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72606\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72607\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72608\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72609\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72610\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72611\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72612\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72613\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72614\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72615\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72616\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72617\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72618\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72619\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72620\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72621\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72622\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72623\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72624\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72625\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72626\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72627\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72628\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72629\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72630\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72631\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72632\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72633\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72634\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72635\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72636\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72637\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72638\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72639\n",
            "Chosing  random actions for the batch :  [1]\n",
            "Experiences :  [Experience(state=array([ 5.11730000e+04,  1.44952857e+00, -7.15116252e-01, -1.02089619e+00,\n",
            "       -1.83609466e+00,  1.40555627e+00,  3.22776439e+00, -1.11883523e+00,\n",
            "        8.00953744e-01, -9.08656330e-01,  6.61084708e-01,  5.08415548e-02,\n",
            "       -6.61676648e-01,  2.19922718e-01, -3.91287852e-02,  9.62713161e-01,\n",
            "        1.54688974e+00, -2.76352025e-01, -1.15033750e+00,  9.87559537e-01,\n",
            "        2.09782176e-01, -1.45720298e-01, -6.97944935e-01,  9.23157885e-02,\n",
            "        9.80218639e-01,  3.92318431e-01, -4.65446451e-01,  1.23893752e-02,\n",
            "        1.71345322e-02,  1.50000000e+01]), action=tensor([0]), reward=1, next_state=array([ 5.11730000e+04,  1.02378008e+00, -5.21147498e-01,  9.52324174e-01,\n",
            "        5.88370333e-01, -1.01250166e+00,  1.13387825e-01, -6.81753279e-01,\n",
            "        2.25388376e-01,  6.89750664e-01, -3.78605619e-02,  6.61466982e-01,\n",
            "        5.36241692e-01, -5.80843261e-01,  1.74568617e-02,  5.96872444e-02,\n",
            "        6.67831633e-01, -6.48711091e-01,  7.13176368e-01,  1.64435409e-01,\n",
            "        5.36588545e-02,  1.63537299e-01,  3.83063141e-01, -1.67957335e-01,\n",
            "        6.31974066e-02,  2.89851390e-01,  5.69038389e-01, -1.86949487e-02,\n",
            "        2.27064901e-02,  8.80000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.1173e+04,  1.4495e+00, -7.1512e-01, -1.0209e+00, -1.8361e+00,\n",
            "          1.4056e+00,  3.2278e+00, -1.1188e+00,  8.0095e-01, -9.0866e-01,\n",
            "          6.6108e-01,  5.0842e-02, -6.6168e-01,  2.1992e-01, -3.9129e-02,\n",
            "          9.6271e-01,  1.5469e+00, -2.7635e-01, -1.1503e+00,  9.8756e-01,\n",
            "          2.0978e-01, -1.4572e-01, -6.9794e-01,  9.2316e-02,  9.8022e-01,\n",
            "          3.9232e-01, -4.6545e-01,  1.2389e-02,  1.7135e-02,  1.5000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.1173e+04,  1.0238e+00, -5.2115e-01,  9.5232e-01,  5.8837e-01,\n",
            "         -1.0125e+00,  1.1339e-01, -6.8175e-01,  2.2539e-01,  6.8975e-01,\n",
            "         -3.7861e-02,  6.6147e-01,  5.3624e-01, -5.8084e-01,  1.7457e-02,\n",
            "          5.9687e-02,  6.6783e-01, -6.4871e-01,  7.1318e-01,  1.6444e-01,\n",
            "          5.3659e-02,  1.6354e-01,  3.8306e-01, -1.6796e-01,  6.3197e-02,\n",
            "          2.8985e-01,  5.6904e-01, -1.8695e-02,  2.2706e-02,  8.8000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72640\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72641\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72642\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72643\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72644\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72645\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72646\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72647\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72648\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72649\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72650\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72651\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72652\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72653\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72654\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72655\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72656\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72657\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72658\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72659\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72660\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72661\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72662\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72663\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72664\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72665\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72666\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72667\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72668\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72669\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72670\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72671\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72672\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72673\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72674\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72675\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72676\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72677\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72678\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72679\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72680\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72681\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72682\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72683\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72684\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72685\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72686\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72687\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72688\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72689\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72690\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72691\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72692\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72693\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72694\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72695\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72696\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72697\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72698\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72699\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72700\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72701\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72702\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72703\n",
            "Chosing  random actions for the batch :  [1]\n",
            "Experiences :  [Experience(state=array([ 3.50230000e+04,  1.07856210e+00,  3.49273153e-01,  8.79176314e-01,\n",
            "        2.47136344e+00, -2.71992945e-01,  5.57272451e-02, -1.52996846e-01,\n",
            "        1.26571351e-01, -4.85028527e-01,  6.29749941e-01, -4.35950501e-01,\n",
            "       -3.63424061e-01, -6.33890919e-01,  3.44352953e-01,  9.88213185e-01,\n",
            "        6.11293083e-01, -3.83858115e-01, -5.81971205e-01, -1.41083741e+00,\n",
            "       -1.69339190e-01, -6.10372837e-02, -2.84345303e-01,  8.86666867e-02,\n",
            "        3.17384358e-02,  1.92634046e-01, -1.00865208e-01,  1.12208799e-02,\n",
            "        3.01049505e-02,  2.99000000e+01]), action=tensor([0]), reward=1, next_state=array([ 3.50230000e+04, -2.29726099e-01,  1.56176313e+00,  1.45028423e+00,\n",
            "        2.52310618e+00,  7.55889113e-01,  5.21132766e-01,  7.05754178e-01,\n",
            "       -1.28951983e+00, -1.05040568e+00,  1.51546636e+00, -8.18534011e-01,\n",
            "        8.41065562e-03,  1.79439273e+00, -5.52000692e-01,  1.17820892e+00,\n",
            "        1.58200820e-04, -5.90374624e-01,  7.69751948e-02,  9.90394325e-01,\n",
            "        3.21167911e-01,  6.17908527e-01, -1.62859930e-01, -1.16459027e-01,\n",
            "       -4.43125563e-01, -2.42861289e-01,  1.04381052e-01, -7.34712374e-02,\n",
            "       -3.01690160e-01,  7.81000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.5023e+04,  1.0786e+00,  3.4927e-01,  8.7918e-01,  2.4714e+00,\n",
            "         -2.7199e-01,  5.5727e-02, -1.5300e-01,  1.2657e-01, -4.8503e-01,\n",
            "          6.2975e-01, -4.3595e-01, -3.6342e-01, -6.3389e-01,  3.4435e-01,\n",
            "          9.8821e-01,  6.1129e-01, -3.8386e-01, -5.8197e-01, -1.4108e+00,\n",
            "         -1.6934e-01, -6.1037e-02, -2.8435e-01,  8.8667e-02,  3.1738e-02,\n",
            "          1.9263e-01, -1.0087e-01,  1.1221e-02,  3.0105e-02,  2.9900e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.5023e+04, -2.2973e-01,  1.5618e+00,  1.4503e+00,  2.5231e+00,\n",
            "          7.5589e-01,  5.2113e-01,  7.0575e-01, -1.2895e+00, -1.0504e+00,\n",
            "          1.5155e+00, -8.1853e-01,  8.4107e-03,  1.7944e+00, -5.5200e-01,\n",
            "          1.1782e+00,  1.5820e-04, -5.9037e-01,  7.6975e-02,  9.9039e-01,\n",
            "          3.2117e-01,  6.1791e-01, -1.6286e-01, -1.1646e-01, -4.4313e-01,\n",
            "         -2.4286e-01,  1.0438e-01, -7.3471e-02, -3.0169e-01,  7.8100e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72704\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72705\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72706\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72707\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72708\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72709\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72710\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72711\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72712\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72713\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72714\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72715\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72716\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72717\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72718\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72719\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72720\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72721\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72722\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72723\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72724\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72725\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72726\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72727\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72728\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72729\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72730\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72731\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72732\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72733\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72734\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72735\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72736\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72737\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72738\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72739\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72740\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72741\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72742\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72743\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72744\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72745\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72746\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72747\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72748\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72749\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72750\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72751\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72752\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72753\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72754\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72755\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72756\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72757\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72758\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72759\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72760\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72761\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72762\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72763\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72764\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72765\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72766\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72767\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.55020000e+04, -1.00952544e+00,  1.56225577e+00,  1.47250661e+00,\n",
            "        2.24182666e+00,  9.38782736e-01,  2.33698614e+00,  8.53769898e-02,\n",
            "        8.36389115e-01, -1.26699179e+00,  1.45277733e+00,  1.86685539e+00,\n",
            "        3.46376816e-01, -3.28414596e-01,  4.93056942e-01,  1.76533098e+00,\n",
            "       -9.79267272e-01,  8.13651304e-01, -1.43332680e+00, -3.40908538e-01,\n",
            "        3.77733279e-01, -1.12968934e-01, -1.85899399e-03,  1.10447925e-01,\n",
            "       -1.41194511e+00, -6.19725738e-01,  1.02868748e-01,  6.60181351e-01,\n",
            "        2.92277110e-01,  1.51000000e+00]), action=tensor([0]), reward=1, next_state=array([ 3.55020000e+04, -5.90786888e-01, -1.84459390e-01,  1.57152547e+00,\n",
            "       -2.37270577e+00, -3.78939657e-01,  3.53066652e-01,  2.09444989e-02,\n",
            "        1.93800215e-01,  1.35243456e+00, -1.81882768e+00,  2.38686252e-01,\n",
            "        1.04316960e+00,  6.26347413e-01, -1.51355543e-01,  1.42098570e+00,\n",
            "       -7.98680510e-01, -3.41188902e-01,  1.60602045e+00,  2.96273303e+00,\n",
            "        4.06050916e-01,  2.32365190e-01,  8.05194990e-01, -4.17644227e-01,\n",
            "       -8.87759680e-01,  6.59901402e-01, -4.00785158e-01,  1.40065569e-01,\n",
            "        7.45602875e-02,  7.79300000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.5502e+04, -1.0095e+00,  1.5623e+00,  1.4725e+00,  2.2418e+00,\n",
            "          9.3878e-01,  2.3370e+00,  8.5377e-02,  8.3639e-01, -1.2670e+00,\n",
            "          1.4528e+00,  1.8669e+00,  3.4638e-01, -3.2841e-01,  4.9306e-01,\n",
            "          1.7653e+00, -9.7927e-01,  8.1365e-01, -1.4333e+00, -3.4091e-01,\n",
            "          3.7773e-01, -1.1297e-01, -1.8590e-03,  1.1045e-01, -1.4119e+00,\n",
            "         -6.1973e-01,  1.0287e-01,  6.6018e-01,  2.9228e-01,  1.5100e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.5502e+04, -5.9079e-01, -1.8446e-01,  1.5715e+00, -2.3727e+00,\n",
            "         -3.7894e-01,  3.5307e-01,  2.0944e-02,  1.9380e-01,  1.3524e+00,\n",
            "         -1.8188e+00,  2.3869e-01,  1.0432e+00,  6.2635e-01, -1.5136e-01,\n",
            "          1.4210e+00, -7.9868e-01, -3.4119e-01,  1.6060e+00,  2.9627e+00,\n",
            "          4.0605e-01,  2.3237e-01,  8.0519e-01, -4.1764e-01, -8.8776e-01,\n",
            "          6.5990e-01, -4.0079e-01,  1.4007e-01,  7.4560e-02,  7.7930e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72768\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72769\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72770\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72771\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72772\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72773\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72774\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72775\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72776\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72777\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72778\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72779\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72780\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72781\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72782\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72783\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72784\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72785\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72786\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72787\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72788\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72789\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72790\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72791\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72792\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72793\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72794\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72795\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72796\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72797\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72798\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72799\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72800\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72801\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72802\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72803\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72804\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72805\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72806\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72807\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72808\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72809\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72810\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72811\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72812\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72813\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72814\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72815\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72816\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72817\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72818\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72819\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72820\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72821\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72822\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72823\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72824\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72825\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72826\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72827\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72828\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72829\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72830\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72831\n",
            "Chosing  random actions for the batch :  [0]\n",
            "Experiences :  [Experience(state=array([ 2.46390000e+04, -6.32780502e-01,  1.16975593e+00,  2.40249408e+00,\n",
            "        2.34885461e+00,  1.24747203e-01,  2.27198372e-01,  6.16037308e-01,\n",
            "       -1.64880448e-01,  8.45138738e-01,  1.01099655e-01,  4.49549462e-01,\n",
            "       -2.87322843e+00,  5.86610676e-01,  1.06693046e+00, -1.51061680e+00,\n",
            "       -5.59015506e-01,  9.99504379e-01, -6.51527457e-01, -1.19890311e+00,\n",
            "       -2.20423584e-01, -1.15463951e-01,  1.45348120e-01, -1.39371389e-01,\n",
            "        5.32431972e-01, -7.95962237e-02, -5.68091507e-02, -3.86311822e-01,\n",
            "       -1.70340845e-01,  1.13400000e+01]), action=[1], reward=0, next_state=array([ 2.46420000e+04,  1.26056119e+00,  4.38431741e-01,  4.40307845e-01,\n",
            "        9.00633387e-01, -3.34956119e-01, -1.03686102e+00, -3.62106470e-02,\n",
            "       -2.59768862e-01,  1.38263348e+00, -6.40245586e-01,  8.08281466e-01,\n",
            "       -2.50057423e+00,  1.43561536e+00,  1.23254282e+00,  4.03546689e-01,\n",
            "        4.69284593e-01,  7.51656433e-01,  2.67169767e-02, -3.98543772e-01,\n",
            "       -1.53196716e-01, -4.43521019e-01, -1.05091527e+00,  1.47006685e-01,\n",
            "        2.72530743e-01,  1.78697011e-01,  5.73297462e-02, -5.24656145e-02,\n",
            "        2.33495371e-02,  9.90000000e-01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 2.4639e+04, -6.3278e-01,  1.1698e+00,  2.4025e+00,  2.3489e+00,\n",
            "          1.2475e-01,  2.2720e-01,  6.1604e-01, -1.6488e-01,  8.4514e-01,\n",
            "          1.0110e-01,  4.4955e-01, -2.8732e+00,  5.8661e-01,  1.0669e+00,\n",
            "         -1.5106e+00, -5.5902e-01,  9.9950e-01, -6.5153e-01, -1.1989e+00,\n",
            "         -2.2042e-01, -1.1546e-01,  1.4535e-01, -1.3937e-01,  5.3243e-01,\n",
            "         -7.9596e-02, -5.6809e-02, -3.8631e-01, -1.7034e-01,  1.1340e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[ 2.4642e+04,  1.2606e+00,  4.3843e-01,  4.4031e-01,  9.0063e-01,\n",
            "         -3.3496e-01, -1.0369e+00, -3.6211e-02, -2.5977e-01,  1.3826e+00,\n",
            "         -6.4025e-01,  8.0828e-01, -2.5006e+00,  1.4356e+00,  1.2325e+00,\n",
            "          4.0355e-01,  4.6928e-01,  7.5166e-01,  2.6717e-02, -3.9854e-01,\n",
            "         -1.5320e-01, -4.4352e-01, -1.0509e+00,  1.4701e-01,  2.7253e-01,\n",
            "          1.7870e-01,  5.7330e-02, -5.2466e-02,  2.3350e-02,  9.9000e-01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72832\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72833\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72834\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72835\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72836\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72837\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72838\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72839\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72840\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72841\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72842\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72843\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72844\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72845\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72846\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72847\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72848\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72849\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72850\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72851\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72852\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72853\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72854\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72855\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72856\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72857\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72858\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72859\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72860\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72861\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72862\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72863\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72864\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72865\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72866\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72867\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72868\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72869\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72870\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72871\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72872\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72873\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72874\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72875\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72876\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72877\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72878\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72879\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72880\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72881\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72882\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72883\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72884\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72885\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72886\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72887\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72888\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72889\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72890\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72891\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72892\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72893\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72894\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72895\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 5.40870000e+04,  1.29629990e+00,  2.44302609e-01, -1.70470182e-01,\n",
            "        3.06712126e-01,  2.19314546e-01, -8.97925113e-02, -6.97393710e-02,\n",
            "        4.16162442e-02, -8.47387969e-02, -9.97778110e-02,  5.17590831e-01,\n",
            "        3.19152478e-01, -6.59793316e-02, -7.91615549e-02,  5.24015986e-01,\n",
            "        1.03485387e+00, -6.57150722e-01,  6.27696849e-01,  5.34303573e-01,\n",
            "       -5.92123103e-02, -3.17739381e-01, -9.66023178e-01, -2.21059785e-02,\n",
            "       -8.89286504e-01,  3.28126651e-01,  1.50821503e-01, -3.10198234e-02,\n",
            "        7.82868494e-03,  1.29000000e+00]), action=tensor([0]), reward=1, next_state=array([ 5.40880000e+04, -1.35190895e+00, -1.49605438e+00,  1.23615278e+00,\n",
            "       -1.95614036e+00, -1.27998557e+00,  2.95496282e-01,  9.40099067e-01,\n",
            "        1.20574911e-01, -1.05353175e+00, -3.94643303e-01, -7.87841094e-02,\n",
            "       -4.85022533e-01, -8.47776506e-01, -2.42882628e-02, -8.09303723e-01,\n",
            "       -6.75281451e-01, -5.69276687e-01,  1.93306636e+00, -6.71713670e-01,\n",
            "        3.81477704e-01, -1.25743630e-01, -7.01143516e-01,  7.31390216e-01,\n",
            "       -3.95387823e-01,  1.03255416e-02,  1.17763568e+00, -1.57690719e-01,\n",
            "        9.69674170e-02,  3.81760000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.4087e+04,  1.2963e+00,  2.4430e-01, -1.7047e-01,  3.0671e-01,\n",
            "          2.1931e-01, -8.9793e-02, -6.9739e-02,  4.1616e-02, -8.4739e-02,\n",
            "         -9.9778e-02,  5.1759e-01,  3.1915e-01, -6.5979e-02, -7.9162e-02,\n",
            "          5.2402e-01,  1.0349e+00, -6.5715e-01,  6.2770e-01,  5.3430e-01,\n",
            "         -5.9212e-02, -3.1774e-01, -9.6602e-01, -2.2106e-02, -8.8929e-01,\n",
            "          3.2813e-01,  1.5082e-01, -3.1020e-02,  7.8287e-03,  1.2900e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.4088e+04, -1.3519e+00, -1.4961e+00,  1.2362e+00, -1.9561e+00,\n",
            "         -1.2800e+00,  2.9550e-01,  9.4010e-01,  1.2057e-01, -1.0535e+00,\n",
            "         -3.9464e-01, -7.8784e-02, -4.8502e-01, -8.4778e-01, -2.4288e-02,\n",
            "         -8.0930e-01, -6.7528e-01, -5.6928e-01,  1.9331e+00, -6.7171e-01,\n",
            "          3.8148e-01, -1.2574e-01, -7.0114e-01,  7.3139e-01, -3.9539e-01,\n",
            "          1.0326e-02,  1.1776e+00, -1.5769e-01,  9.6967e-02,  3.8176e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72896\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72897\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72898\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72899\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72900\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72901\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72902\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72903\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72904\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72905\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72906\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72907\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72908\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72909\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72910\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72911\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72912\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72913\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72914\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72915\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72916\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72917\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72918\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72919\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72920\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72921\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72922\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72923\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72924\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72925\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72926\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72927\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72928\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72929\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72930\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72931\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72932\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72933\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72934\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72935\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72936\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72937\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72938\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72939\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72940\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72941\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72942\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72943\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72944\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72945\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72946\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72947\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72948\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72949\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72950\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72951\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72952\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72953\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72954\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72955\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72956\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72957\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72958\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72959\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 4.77190000e+04,  1.19144898e+00, -4.29850526e-02,  8.39853331e-01,\n",
            "        9.45714185e-01, -2.72046067e-01,  8.08620994e-01, -6.19338133e-01,\n",
            "        2.55105292e-01,  5.62925215e-01, -6.02860846e-02,  3.33017332e-02,\n",
            "        1.31029392e+00,  1.13756389e+00, -4.28627366e-01, -5.45568286e-01,\n",
            "        5.68005436e-01, -9.78778877e-01,  6.92155015e-01,  5.10787514e-01,\n",
            "       -2.13202412e-02, -7.56262626e-02,  5.62441543e-03, -1.73915689e-01,\n",
            "       -8.13069500e-01,  5.73328910e-01, -3.57077316e-01,  7.35373401e-02,\n",
            "        1.84214493e-02,  9.99000000e+00]), action=[1], reward=0, next_state=array([ 4.77190000e+04,  1.37999868e+00, -9.01171218e-01,  3.44600798e-01,\n",
            "       -1.89521147e+00, -1.44898894e+00, -1.13128100e+00, -5.68423995e-01,\n",
            "       -2.02007376e-01,  6.55055017e-01, -3.93754411e-01, -6.87092032e-01,\n",
            "        4.64005431e-01, -6.99497569e-02, -1.35282869e-01,  4.38997954e-01,\n",
            "       -2.43519669e+00,  5.27320802e-01,  8.20132637e-01,  4.00766319e-01,\n",
            "       -4.97806618e-01, -6.36824347e-01, -1.13972499e+00,  1.00093619e-01,\n",
            "        3.83049544e-01,  3.50360758e-01, -1.89536589e-01,  3.94905635e-02,\n",
            "        2.24750991e-02,  1.90100000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 4.7719e+04,  1.1914e+00, -4.2985e-02,  8.3985e-01,  9.4571e-01,\n",
            "         -2.7205e-01,  8.0862e-01, -6.1934e-01,  2.5511e-01,  5.6293e-01,\n",
            "         -6.0286e-02,  3.3302e-02,  1.3103e+00,  1.1376e+00, -4.2863e-01,\n",
            "         -5.4557e-01,  5.6801e-01, -9.7878e-01,  6.9216e-01,  5.1079e-01,\n",
            "         -2.1320e-02, -7.5626e-02,  5.6244e-03, -1.7392e-01, -8.1307e-01,\n",
            "          5.7333e-01, -3.5708e-01,  7.3537e-02,  1.8421e-02,  9.9900e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[ 4.7719e+04,  1.3800e+00, -9.0117e-01,  3.4460e-01, -1.8952e+00,\n",
            "         -1.4490e+00, -1.1313e+00, -5.6842e-01, -2.0201e-01,  6.5506e-01,\n",
            "         -3.9375e-01, -6.8709e-01,  4.6401e-01, -6.9950e-02, -1.3528e-01,\n",
            "          4.3900e-01, -2.4352e+00,  5.2732e-01,  8.2013e-01,  4.0077e-01,\n",
            "         -4.9781e-01, -6.3682e-01, -1.1397e+00,  1.0009e-01,  3.8305e-01,\n",
            "          3.5036e-01, -1.8954e-01,  3.9491e-02,  2.2475e-02,  1.9010e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  72960\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72961\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72962\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72963\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72964\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72965\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72966\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72967\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72968\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72969\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72970\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72971\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72972\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72973\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72974\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72975\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72976\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72977\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72978\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72979\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72980\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72981\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72982\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  72983\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72984\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72985\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72986\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72987\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72988\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72989\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72990\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  72991\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72992\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72993\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72994\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72995\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72996\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72997\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72998\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  72999\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73000\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73001\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73002\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73003\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73004\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73005\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73006\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73007\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73008\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73009\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73010\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73011\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73012\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73013\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73014\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73015\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73016\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73017\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73018\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73019\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73020\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73021\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73022\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73023\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.06680000e+04, -1.41705345e+00,  3.18701395e-01,  2.10325404e+00,\n",
            "        1.04441641e+00, -1.45950960e+00,  9.99080063e-01,  3.67061673e-01,\n",
            "        8.15901738e-01, -5.29055203e-01, -8.31659386e-01,  1.11998174e+00,\n",
            "        6.26868574e-01, -5.36506274e-01,  3.90829138e-01,  4.26524513e-01,\n",
            "       -6.72792025e-02,  1.83114457e-01,  5.16051366e-01,  3.23400278e-01,\n",
            "        4.06314673e-01,  3.20269845e-01,  4.62036583e-01,  3.85570392e-01,\n",
            "        1.79507510e-01,  1.90608358e-01, -2.69978118e-01, -7.18888988e-02,\n",
            "        3.52097831e-02,  2.47530000e+02]), action=tensor([0]), reward=1, next_state=array([ 3.06680000e+04, -2.28120351e-01,  1.26556861e+00,  9.72518974e-01,\n",
            "        8.63821755e-01,  5.58506423e-01, -9.79832207e-01,  1.19967249e+00,\n",
            "       -3.44276169e-01, -1.09600594e+00, -2.33612834e-01, -3.17273238e-01,\n",
            "        4.26390452e-01,  1.02275757e+00,  3.20120131e-01,  6.79274315e-01,\n",
            "       -6.41317321e-01, -1.23317258e-01, -2.30843117e-01,  5.52437816e-01,\n",
            "        6.45284901e-02,  1.40265743e-01,  5.01821574e-01, -2.50813958e-01,\n",
            "        4.39672574e-01,  2.81780785e-02, -2.90970961e-01,  1.46209819e-01,\n",
            "        1.47695901e-01,  5.43000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.0668e+04, -1.4171e+00,  3.1870e-01,  2.1033e+00,  1.0444e+00,\n",
            "         -1.4595e+00,  9.9908e-01,  3.6706e-01,  8.1590e-01, -5.2906e-01,\n",
            "         -8.3166e-01,  1.1200e+00,  6.2687e-01, -5.3651e-01,  3.9083e-01,\n",
            "          4.2652e-01, -6.7279e-02,  1.8311e-01,  5.1605e-01,  3.2340e-01,\n",
            "          4.0631e-01,  3.2027e-01,  4.6204e-01,  3.8557e-01,  1.7951e-01,\n",
            "          1.9061e-01, -2.6998e-01, -7.1889e-02,  3.5210e-02,  2.4753e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.0668e+04, -2.2812e-01,  1.2656e+00,  9.7252e-01,  8.6382e-01,\n",
            "          5.5851e-01, -9.7983e-01,  1.1997e+00, -3.4428e-01, -1.0960e+00,\n",
            "         -2.3361e-01, -3.1727e-01,  4.2639e-01,  1.0228e+00,  3.2012e-01,\n",
            "          6.7927e-01, -6.4132e-01, -1.2332e-01, -2.3084e-01,  5.5244e-01,\n",
            "          6.4528e-02,  1.4027e-01,  5.0182e-01, -2.5081e-01,  4.3967e-01,\n",
            "          2.8178e-02, -2.9097e-01,  1.4621e-01,  1.4770e-01,  5.4300e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73024\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73025\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73026\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73027\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73028\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73029\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73030\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73031\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73032\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73033\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73034\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73035\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73036\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73037\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73038\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73039\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73040\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73041\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73042\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73043\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73044\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73045\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73046\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73047\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73048\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73049\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73050\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73051\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73052\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73053\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73054\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73055\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73056\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73057\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73058\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73059\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73060\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73061\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73062\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73063\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73064\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73065\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73066\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73067\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73068\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73069\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73070\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73071\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73072\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73073\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73074\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73075\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73076\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73077\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73078\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73079\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73080\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73081\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73082\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73083\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73084\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73085\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73086\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73087\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.56590000e+04, -3.27304565e+00, -3.50755850e+00,  8.87629555e-01,\n",
            "        9.78204398e-02,  4.80915493e-01, -1.08129087e+00, -9.45019360e-01,\n",
            "        8.42355005e-01,  5.76849171e-01, -1.16608636e+00, -1.73485061e-02,\n",
            "        7.61585755e-01, -7.89005498e-01,  1.99690867e-01, -1.50593283e+00,\n",
            "        5.36190949e-01, -2.53599039e-01,  4.12872870e-01,  2.16387746e-01,\n",
            "        1.20396177e+00,  4.12847760e-01, -3.47711777e-01,  7.14226895e-01,\n",
            "        2.81950599e-02, -8.78662491e-01,  6.37466469e-01, -4.62544891e-02,\n",
            "       -3.45598615e-01,  2.85560000e+02]), action=tensor([0]), reward=1, next_state=array([ 3.56590000e+04,  1.08001490e+00,  9.08866177e-03,  4.64720948e-01,\n",
            "        1.35041033e+00, -3.36806478e-01, -2.53959164e-03, -1.15170514e-01,\n",
            "        2.01568055e-01,  2.45215601e-01,  8.20143977e-02,  1.08546863e+00,\n",
            "        4.79587913e-01, -1.76489558e+00,  5.79334163e-01, -6.41207832e-01,\n",
            "       -5.13015064e-01,  1.73694990e-01, -3.60269905e-01, -1.48846713e-01,\n",
            "       -2.54997903e-01, -4.59019983e-02, -3.79132871e-02, -4.31666716e-02,\n",
            "        2.04126753e-01,  5.49694942e-01, -3.39241766e-01,  2.38291311e-02,\n",
            "        7.04969552e-03,  2.20800000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.5659e+04, -3.2730e+00, -3.5076e+00,  8.8763e-01,  9.7820e-02,\n",
            "          4.8092e-01, -1.0813e+00, -9.4502e-01,  8.4236e-01,  5.7685e-01,\n",
            "         -1.1661e+00, -1.7349e-02,  7.6159e-01, -7.8901e-01,  1.9969e-01,\n",
            "         -1.5059e+00,  5.3619e-01, -2.5360e-01,  4.1287e-01,  2.1639e-01,\n",
            "          1.2040e+00,  4.1285e-01, -3.4771e-01,  7.1423e-01,  2.8195e-02,\n",
            "         -8.7866e-01,  6.3747e-01, -4.6254e-02, -3.4560e-01,  2.8556e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.5659e+04,  1.0800e+00,  9.0887e-03,  4.6472e-01,  1.3504e+00,\n",
            "         -3.3681e-01, -2.5396e-03, -1.1517e-01,  2.0157e-01,  2.4522e-01,\n",
            "          8.2014e-02,  1.0855e+00,  4.7959e-01, -1.7649e+00,  5.7933e-01,\n",
            "         -6.4121e-01, -5.1302e-01,  1.7369e-01, -3.6027e-01, -1.4885e-01,\n",
            "         -2.5500e-01, -4.5902e-02, -3.7913e-02, -4.3167e-02,  2.0413e-01,\n",
            "          5.4969e-01, -3.3924e-01,  2.3829e-02,  7.0497e-03,  2.2080e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73088\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73089\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73090\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73091\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73092\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73093\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73094\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73095\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73096\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73097\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73098\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73099\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73100\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73101\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73102\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73103\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73104\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73105\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73106\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73107\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73108\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73109\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73110\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73111\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73112\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73113\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73114\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73115\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73116\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73117\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73118\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73119\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73120\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73121\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73122\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73123\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73124\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73125\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73126\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73127\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73128\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73129\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73130\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73131\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73132\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73133\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73134\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73135\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73136\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73137\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73138\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73139\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73140\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73141\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73142\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73143\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73144\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73145\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73146\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73147\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73148\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73149\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73150\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73151\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 5.18490000e+04, -1.93882632e+00, -1.43754834e+00,  1.60731253e+00,\n",
            "        1.29295891e+00,  4.64807519e-01,  9.18145218e-01,  5.77993011e-01,\n",
            "       -3.69024564e-01,  1.03712577e+00,  3.34932584e-01, -1.26192650e+00,\n",
            "       -3.88451188e-01,  6.49062311e-02, -1.05348376e+00,  9.63891150e-01,\n",
            "       -2.47986085e-01, -3.84218976e-01, -2.22744109e-02,  7.46489133e-01,\n",
            "       -7.07161010e-01, -4.05356376e-01,  3.12192005e-01,  5.90742126e-01,\n",
            "       -8.63927197e-01,  1.49771317e-01, -2.86862697e-01, -2.89055057e-01,\n",
            "       -3.13073068e-01,  1.90000000e+02]), action=tensor([0]), reward=1, next_state=array([ 5.18490000e+04, -3.61726178e-01, -3.20723069e-01,  1.78631126e+00,\n",
            "        1.44776941e+00, -5.73660517e-01,  1.66682467e+00, -1.83617858e-01,\n",
            "        6.23077955e-01,  5.51506345e-01, -3.70459221e-01,  8.68662157e-01,\n",
            "        1.51068098e+00,  3.03717436e-01, -6.31144538e-01, -1.15818935e+00,\n",
            "       -1.00109551e+00,  5.28442021e-01, -3.57928180e-01,  2.13300363e-01,\n",
            "        2.42946807e-01,  3.00889080e-01,  1.04967365e+00,  5.35530617e-01,\n",
            "       -2.60392231e-01, -1.22490794e+00, -4.16000153e-01,  2.71625058e-01,\n",
            "        1.93457959e-01,  1.50000000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.1849e+04, -1.9388e+00, -1.4375e+00,  1.6073e+00,  1.2930e+00,\n",
            "          4.6481e-01,  9.1815e-01,  5.7799e-01, -3.6902e-01,  1.0371e+00,\n",
            "          3.3493e-01, -1.2619e+00, -3.8845e-01,  6.4906e-02, -1.0535e+00,\n",
            "          9.6389e-01, -2.4799e-01, -3.8422e-01, -2.2274e-02,  7.4649e-01,\n",
            "         -7.0716e-01, -4.0536e-01,  3.1219e-01,  5.9074e-01, -8.6393e-01,\n",
            "          1.4977e-01, -2.8686e-01, -2.8906e-01, -3.1307e-01,  1.9000e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.1849e+04, -3.6173e-01, -3.2072e-01,  1.7863e+00,  1.4478e+00,\n",
            "         -5.7366e-01,  1.6668e+00, -1.8362e-01,  6.2308e-01,  5.5151e-01,\n",
            "         -3.7046e-01,  8.6866e-01,  1.5107e+00,  3.0372e-01, -6.3114e-01,\n",
            "         -1.1582e+00, -1.0011e+00,  5.2844e-01, -3.5793e-01,  2.1330e-01,\n",
            "          2.4295e-01,  3.0089e-01,  1.0497e+00,  5.3553e-01, -2.6039e-01,\n",
            "         -1.2249e+00, -4.1600e-01,  2.7163e-01,  1.9346e-01,  1.5000e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73152\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73153\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73154\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73155\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73156\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73157\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73158\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73159\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73160\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73161\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73162\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73163\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73164\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73165\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73166\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73167\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73168\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73169\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73170\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73171\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73172\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73173\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73174\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73175\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73176\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73177\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73178\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73179\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73180\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73181\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73182\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73183\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73184\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73185\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73186\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73187\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73188\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73189\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73190\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73191\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73192\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73193\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73194\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73195\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73196\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73197\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73198\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73199\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73200\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73201\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73202\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73203\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73204\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73205\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73206\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73207\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73208\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73209\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73210\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73211\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73212\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73213\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73214\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73215\n",
            "Chosing  random actions for the batch :  [1]\n",
            "Experiences :  [Experience(state=array([ 2.91740000e+04, -1.00653417e+00,  8.20628287e-01,  4.45040959e-01,\n",
            "       -2.11964659e+00,  6.72291729e-02, -6.99762274e-01,  1.36400569e+00,\n",
            "       -1.13602487e+00, -2.13093025e-03, -1.31361432e+00,  1.17008678e+00,\n",
            "        1.10675496e+00,  2.05481428e-01,  5.18233779e-01,  6.66520995e-02,\n",
            "       -3.72360462e-01, -6.42258265e-01,  3.69423708e-01,  5.21024470e-01,\n",
            "       -3.85838264e-01,  8.05029836e-01, -1.07245778e-02, -4.40279667e-01,\n",
            "        4.41364952e-02,  1.04833247e+00, -5.28495823e-01, -2.06648946e-02,\n",
            "       -1.07355693e-01,  1.32030000e+02]), action=[0], reward=0, next_state=array([ 2.91760000e+04, -5.80794495e-01,  1.31918072e+00,  9.20253479e-01,\n",
            "        9.31453691e-01, -2.05309918e-01, -7.51335371e-01,  6.13420124e-01,\n",
            "        2.03765276e-01, -9.57629080e-01, -4.91984242e-01, -5.46013213e-01,\n",
            "        4.22743080e-01,  8.41363234e-01,  5.13624977e-01,  8.90071982e-01,\n",
            "       -2.33987253e-01, -6.46344253e-02,  6.74950531e-02,  4.14467096e-01,\n",
            "       -4.76095111e-03,  2.25428447e-01,  5.60166760e-01, -5.35788854e-02,\n",
            "        4.19325931e-01, -1.96576072e-01, -3.36163768e-01,  4.34764413e-02,\n",
            "        1.05726474e-01,  3.70000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 2.9174e+04, -1.0065e+00,  8.2063e-01,  4.4504e-01, -2.1196e+00,\n",
            "          6.7229e-02, -6.9976e-01,  1.3640e+00, -1.1360e+00, -2.1309e-03,\n",
            "         -1.3136e+00,  1.1701e+00,  1.1068e+00,  2.0548e-01,  5.1823e-01,\n",
            "          6.6652e-02, -3.7236e-01, -6.4226e-01,  3.6942e-01,  5.2102e-01,\n",
            "         -3.8584e-01,  8.0503e-01, -1.0725e-02, -4.4028e-01,  4.4136e-02,\n",
            "          1.0483e+00, -5.2850e-01, -2.0665e-02, -1.0736e-01,  1.3203e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[ 2.9176e+04, -5.8079e-01,  1.3192e+00,  9.2025e-01,  9.3145e-01,\n",
            "         -2.0531e-01, -7.5134e-01,  6.1342e-01,  2.0377e-01, -9.5763e-01,\n",
            "         -4.9198e-01, -5.4601e-01,  4.2274e-01,  8.4136e-01,  5.1362e-01,\n",
            "          8.9007e-01, -2.3399e-01, -6.4634e-02,  6.7495e-02,  4.1447e-01,\n",
            "         -4.7610e-03,  2.2543e-01,  5.6017e-01, -5.3579e-02,  4.1933e-01,\n",
            "         -1.9658e-01, -3.3616e-01,  4.3476e-02,  1.0573e-01,  3.7000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(1.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73216\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73217\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73218\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73219\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73220\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73221\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73222\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73223\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73224\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73225\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73226\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73227\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73228\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73229\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73230\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73231\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73232\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73233\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73234\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73235\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73236\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73237\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73238\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73239\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73240\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73241\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73242\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73243\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73244\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73245\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73246\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73247\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73248\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73249\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73250\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73251\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73252\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73253\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73254\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73255\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73256\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73257\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73258\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73259\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73260\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73261\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73262\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73263\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73264\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73265\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73266\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73267\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73268\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73269\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73270\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73271\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73272\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73273\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73274\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73275\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73276\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73277\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73278\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73279\n",
            "Chosing  random actions for the batch :  [0]\n",
            "Experiences :  [Experience(state=array([ 5.57880000e+04,  1.22397616e+00,  2.90628845e-01,  7.43957820e-01,\n",
            "        7.42546595e-01, -7.09721585e-01, -1.29021147e+00,  9.55616048e-02,\n",
            "       -2.58812220e-01, -4.25316963e-02, -7.61714594e-02,  3.18330047e-01,\n",
            "        7.24653147e-01,  5.59194674e-01,  2.03983921e-01,  9.55784892e-01,\n",
            "        1.31486569e-01, -2.48588562e-01, -7.57711067e-01, -3.21879285e-01,\n",
            "       -8.99314002e-02, -2.18829588e-01, -6.18373805e-01,  1.98738218e-01,\n",
            "        9.21868991e-01,  1.57947580e-01,  6.04777029e-02, -2.60568525e-02,\n",
            "        2.50602314e-02,  1.29000000e+00]), action=tensor([0]), reward=1, next_state=array([ 5.57890000e+04, -1.64437784e+00, -9.87392275e-01,  1.27432931e+00,\n",
            "       -1.99435777e-01,  2.10552562e+00,  4.51094875e+00, -8.37917794e-01,\n",
            "        1.33931758e+00,  1.11798099e+00, -9.21152153e-01, -1.47142647e+00,\n",
            "        2.96960040e-01, -3.63534270e-01, -1.11901162e+00, -1.97507467e+00,\n",
            "       -5.66439060e-05, -4.57652410e-01,  3.52902142e-01,  5.16552760e-02,\n",
            "       -3.70342587e-02, -3.32230032e-02,  1.70598284e-01,  1.70480979e-01,\n",
            "        1.02779885e+00,  4.68226874e-01, -3.58127958e-01,  7.69705609e-02,\n",
            "        1.72914266e-01,  1.01590000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.5788e+04,  1.2240e+00,  2.9063e-01,  7.4396e-01,  7.4255e-01,\n",
            "         -7.0972e-01, -1.2902e+00,  9.5562e-02, -2.5881e-01, -4.2532e-02,\n",
            "         -7.6171e-02,  3.1833e-01,  7.2465e-01,  5.5919e-01,  2.0398e-01,\n",
            "          9.5578e-01,  1.3149e-01, -2.4859e-01, -7.5771e-01, -3.2188e-01,\n",
            "         -8.9931e-02, -2.1883e-01, -6.1837e-01,  1.9874e-01,  9.2187e-01,\n",
            "          1.5795e-01,  6.0478e-02, -2.6057e-02,  2.5060e-02,  1.2900e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.5789e+04, -1.6444e+00, -9.8739e-01,  1.2743e+00, -1.9944e-01,\n",
            "          2.1055e+00,  4.5109e+00, -8.3792e-01,  1.3393e+00,  1.1180e+00,\n",
            "         -9.2115e-01, -1.4714e+00,  2.9696e-01, -3.6353e-01, -1.1190e+00,\n",
            "         -1.9751e+00, -5.6644e-05, -4.5765e-01,  3.5290e-01,  5.1655e-02,\n",
            "         -3.7034e-02, -3.3223e-02,  1.7060e-01,  1.7048e-01,  1.0278e+00,\n",
            "          4.6823e-01, -3.5813e-01,  7.6971e-02,  1.7291e-01,  1.0159e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73280\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73281\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73282\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73283\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73284\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73285\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73286\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73287\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73288\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73289\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73290\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73291\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73292\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73293\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73294\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73295\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73296\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73297\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73298\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73299\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73300\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73301\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73302\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73303\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73304\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73305\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73306\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73307\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73308\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73309\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73310\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73311\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73312\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73313\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73314\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73315\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73316\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73317\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73318\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73319\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73320\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73321\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73322\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73323\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73324\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73325\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73326\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73327\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73328\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73329\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73330\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73331\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73332\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73333\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73334\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73335\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73336\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73337\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73338\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73339\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73340\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73341\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73342\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73343\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 8.44000000e+03, -1.64408754e+00, -6.22457964e-01,  1.43476433e+00,\n",
            "       -1.75829452e+00, -1.20067245e-01,  1.14650177e+00, -9.86688852e-01,\n",
            "        9.42482598e-01,  4.28423619e-01, -5.90428698e-01,  2.70278197e-01,\n",
            "       -3.33034314e+00,  1.25526381e+00,  1.17759263e+00, -1.19490042e+00,\n",
            "        1.88317246e+00,  4.44687492e-01,  1.33128715e-01,  2.81416295e-01,\n",
            "        4.55415254e-01,  2.71592005e-01,  5.68053675e-01, -2.97595235e-01,\n",
            "       -1.76202833e+00,  6.67187163e-01, -3.65460972e-02,  1.40628520e-01,\n",
            "       -6.78528695e-02,  1.07100000e+02]), action=tensor([1]), reward=-1, next_state=array([ 8.44200000e+03,  1.31882900e+00,  7.95981169e-02, -9.47571181e-01,\n",
            "       -3.53668819e-01,  2.10461564e+00,  3.31509505e+00, -6.28005711e-01,\n",
            "        7.27754687e-01,  1.23225713e+00, -3.69337095e-01,  9.18943339e-01,\n",
            "       -2.43327387e+00,  1.68106536e+00,  1.86011091e+00,  5.07793609e-01,\n",
            "        6.11090909e-01, -1.77927687e-01,  2.04646037e-01,  5.21051152e-02,\n",
            "       -2.67019490e-02, -4.63924906e-01, -1.25507284e+00,  1.15733152e-01,\n",
            "        9.31089055e-01,  3.46348479e-01,  7.17950070e-02, -5.49989164e-02,\n",
            "        5.38997946e-03,  1.79000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 8.4400e+03, -1.6441e+00, -6.2246e-01,  1.4348e+00, -1.7583e+00,\n",
            "         -1.2007e-01,  1.1465e+00, -9.8669e-01,  9.4248e-01,  4.2842e-01,\n",
            "         -5.9043e-01,  2.7028e-01, -3.3303e+00,  1.2553e+00,  1.1776e+00,\n",
            "         -1.1949e+00,  1.8832e+00,  4.4469e-01,  1.3313e-01,  2.8142e-01,\n",
            "          4.5542e-01,  2.7159e-01,  5.6805e-01, -2.9760e-01, -1.7620e+00,\n",
            "          6.6719e-01, -3.6546e-02,  1.4063e-01, -6.7853e-02,  1.0710e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[-1.]], device='cuda:0', dtype=torch.float64), tensor([[ 8.4420e+03,  1.3188e+00,  7.9598e-02, -9.4757e-01, -3.5367e-01,\n",
            "          2.1046e+00,  3.3151e+00, -6.2801e-01,  7.2775e-01,  1.2323e+00,\n",
            "         -3.6934e-01,  9.1894e-01, -2.4333e+00,  1.6811e+00,  1.8601e+00,\n",
            "          5.0779e-01,  6.1109e-01, -1.7793e-01,  2.0465e-01,  5.2105e-02,\n",
            "         -2.6702e-02, -4.6392e-01, -1.2551e+00,  1.1573e-01,  9.3109e-01,\n",
            "          3.4635e-01,  7.1795e-02, -5.4999e-02,  5.3900e-03,  1.7900e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[0.9356]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-5.2827, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73344\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73345\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73346\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73347\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73348\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73349\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73350\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73351\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73352\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73353\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73354\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73355\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73356\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73357\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73358\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73359\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73360\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73361\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73362\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73363\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73364\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73365\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73366\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73367\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73368\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73369\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73370\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73371\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73372\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73373\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73374\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73375\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73376\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73377\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73378\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73379\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73380\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73381\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73382\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73383\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73384\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73385\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73386\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73387\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73388\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73389\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73390\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73391\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73392\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73393\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73394\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73395\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73396\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73397\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73398\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73399\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73400\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73401\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73402\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73403\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73404\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73405\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73406\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73407\n",
            "Chosing  random actions for the batch :  [0]\n",
            "Experiences :  [Experience(state=array([ 5.08850000e+04,  1.10895744e+00, -1.75184241e-01,  7.96627041e-01,\n",
            "        1.03635677e+00, -1.47851766e-01,  1.29959036e+00, -7.17396179e-01,\n",
            "        4.34087291e-01,  5.97337904e-01, -3.64484656e-02,  1.23362726e-01,\n",
            "        1.00706429e+00,  4.36077042e-01, -2.63221615e-01, -2.55129282e-01,\n",
            "        3.52284640e-01, -6.75595304e-01,  3.23383231e-01,  9.73633790e-02,\n",
            "       -6.18419203e-02, -4.59473370e-02,  4.60243590e-02, -1.49351303e-01,\n",
            "       -1.14374499e+00,  4.64043875e-01, -3.11143786e-01,  8.06588692e-02,\n",
            "        1.67919758e-02,  2.79800000e+01]), action=tensor([0]), reward=1, next_state=array([ 5.08850000e+04, -1.43548179e+00,  1.37732557e+00,  1.12119202e+00,\n",
            "        1.28388863e-01, -4.53566068e-01, -6.77864873e-01,  2.98538155e-01,\n",
            "        3.53553245e-01,  1.15242000e-01, -1.66741895e-01, -9.53382161e-01,\n",
            "        2.65484129e-01, -1.34262450e-01, -3.12143385e-03, -4.15691148e-01,\n",
            "       -3.33621709e-01,  2.03158300e-01, -5.15448072e-01,  3.66939301e-01,\n",
            "        5.40102946e-02, -1.50385430e-01, -3.13230371e-01, -3.50230767e-02,\n",
            "        4.10142282e-01,  4.82253624e-02,  2.22082892e-01,  1.70327809e-02,\n",
            "        2.05880608e-01,  9.99000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.0885e+04,  1.1090e+00, -1.7518e-01,  7.9663e-01,  1.0364e+00,\n",
            "         -1.4785e-01,  1.2996e+00, -7.1740e-01,  4.3409e-01,  5.9734e-01,\n",
            "         -3.6448e-02,  1.2336e-01,  1.0071e+00,  4.3608e-01, -2.6322e-01,\n",
            "         -2.5513e-01,  3.5228e-01, -6.7560e-01,  3.2338e-01,  9.7363e-02,\n",
            "         -6.1842e-02, -4.5947e-02,  4.6024e-02, -1.4935e-01, -1.1437e+00,\n",
            "          4.6404e-01, -3.1114e-01,  8.0659e-02,  1.6792e-02,  2.7980e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.0885e+04, -1.4355e+00,  1.3773e+00,  1.1212e+00,  1.2839e-01,\n",
            "         -4.5357e-01, -6.7786e-01,  2.9854e-01,  3.5355e-01,  1.1524e-01,\n",
            "         -1.6674e-01, -9.5338e-01,  2.6548e-01, -1.3426e-01, -3.1214e-03,\n",
            "         -4.1569e-01, -3.3362e-01,  2.0316e-01, -5.1545e-01,  3.6694e-01,\n",
            "          5.4010e-02, -1.5039e-01, -3.1323e-01, -3.5023e-02,  4.1014e-01,\n",
            "          4.8225e-02,  2.2208e-01,  1.7033e-02,  2.0588e-01,  9.9900e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73408\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73409\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73410\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73411\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73412\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73413\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73414\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73415\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73416\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73417\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73418\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73419\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73420\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73421\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73422\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73423\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73424\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73425\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73426\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73427\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73428\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73429\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73430\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73431\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73432\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73433\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73434\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73435\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73436\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73437\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73438\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73439\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73440\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73441\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73442\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73443\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73444\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73445\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73446\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73447\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73448\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73449\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73450\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73451\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73452\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73453\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73454\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73455\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73456\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73457\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73458\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73459\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73460\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73461\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73462\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73463\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73464\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73465\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73466\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73467\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73468\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73469\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73470\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73471\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 5.70800000e+04,  1.01898718e+00, -8.06000961e-01,  1.13335615e+00,\n",
            "        1.91068854e-02, -9.89046484e-01,  1.04538437e+00, -1.07209656e+00,\n",
            "        5.43776323e-01,  1.26341299e+00, -4.15631391e-01,  1.04685442e+00,\n",
            "        1.20639446e+00, -6.78358450e-01, -4.53290104e-01, -9.95689766e-01,\n",
            "       -6.80745375e-01,  6.92027147e-01, -9.37517845e-01,  1.19585220e-01,\n",
            "       -9.16373196e-02,  5.89223395e-02,  4.92507117e-01, -3.37340176e-02,\n",
            "       -1.46737543e-01,  8.22834055e-02,  1.58574308e+00, -3.09034087e-02,\n",
            "       -7.56050095e-03,  3.97800000e+01]), action=tensor([0]), reward=1, next_state=array([ 5.70810000e+04,  3.43022804e-01, -2.14866149e+00, -9.90381485e-01,\n",
            "       -5.39851444e-01,  8.54861189e-01,  4.04436625e+00, -5.99354833e-01,\n",
            "        9.30135662e-01,  1.01933622e+00, -6.13941266e-01, -5.62882013e-01,\n",
            "        4.14351859e-01, -8.72002514e-04, -3.03263366e-01, -5.35486291e-02,\n",
            "        2.69735634e-01, -3.40053101e-01, -9.46641727e-02,  6.85419793e-01,\n",
            "        1.01919269e+00,  2.46002900e-03, -9.70912683e-01, -3.76710918e-01,\n",
            "        1.10186533e+00,  1.19649288e-01,  9.06848306e-01, -1.21807290e-01,\n",
            "        9.08007508e-02,  4.86740000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.7080e+04,  1.0190e+00, -8.0600e-01,  1.1334e+00,  1.9107e-02,\n",
            "         -9.8905e-01,  1.0454e+00, -1.0721e+00,  5.4378e-01,  1.2634e+00,\n",
            "         -4.1563e-01,  1.0469e+00,  1.2064e+00, -6.7836e-01, -4.5329e-01,\n",
            "         -9.9569e-01, -6.8075e-01,  6.9203e-01, -9.3752e-01,  1.1959e-01,\n",
            "         -9.1637e-02,  5.8922e-02,  4.9251e-01, -3.3734e-02, -1.4674e-01,\n",
            "          8.2283e-02,  1.5857e+00, -3.0903e-02, -7.5605e-03,  3.9780e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.7081e+04,  3.4302e-01, -2.1487e+00, -9.9038e-01, -5.3985e-01,\n",
            "          8.5486e-01,  4.0444e+00, -5.9935e-01,  9.3014e-01,  1.0193e+00,\n",
            "         -6.1394e-01, -5.6288e-01,  4.1435e-01, -8.7200e-04, -3.0326e-01,\n",
            "         -5.3549e-02,  2.6974e-01, -3.4005e-01, -9.4664e-02,  6.8542e-01,\n",
            "          1.0192e+00,  2.4600e-03, -9.7091e-01, -3.7671e-01,  1.1019e+00,\n",
            "          1.1965e-01,  9.0685e-01, -1.2181e-01,  9.0801e-02,  4.8674e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73472\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73473\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73474\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73475\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73476\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73477\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73478\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73479\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73480\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73481\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73482\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73483\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73484\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73485\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73486\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73487\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73488\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73489\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73490\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73491\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73492\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73493\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73494\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73495\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73496\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73497\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73498\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73499\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73500\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73501\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73502\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73503\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73504\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73505\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73506\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73507\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73508\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73509\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73510\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73511\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73512\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73513\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73514\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73515\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73516\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73517\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73518\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73519\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73520\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73521\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73522\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73523\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73524\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73525\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73526\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73527\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73528\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73529\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73530\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73531\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73532\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73533\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73534\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73535\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 5.06800000e+04, -1.32132828e+00,  7.36156753e-01,  5.52139761e-01,\n",
            "       -8.60286254e-01, -2.76342856e-01, -1.12067517e+00,  1.57074893e+00,\n",
            "       -2.93948142e-01, -6.46295487e-01, -6.79934436e-01, -4.19360900e-01,\n",
            "       -3.10170317e-02,  1.10400975e-01,  3.00606379e-01,  2.47261560e-01,\n",
            "        1.79431121e-01, -3.62714522e-01, -6.33096278e-01, -3.98014630e-01,\n",
            "       -2.40252276e-01, -4.60559413e-02, -1.16314022e-01, -3.24839443e-01,\n",
            "        4.87648010e-01,  6.19038715e-01,  9.47099799e-01, -6.02063669e-01,\n",
            "       -4.06202294e-01,  1.54950000e+02]), action=tensor([0]), reward=1, next_state=array([ 5.06810000e+04,  1.02283893e+00, -7.65813684e-01,  8.14834217e-01,\n",
            "       -2.77068216e-01, -1.05868460e+00, -6.64520817e-02, -6.58216390e-01,\n",
            "        6.64830008e-04,  7.86134939e-01, -3.29248360e-01, -9.00766700e-01,\n",
            "        3.71599395e-01,  1.41055512e+00, -4.88683215e-01,  1.65971536e+00,\n",
            "        1.12610454e+00, -9.22139248e-01,  3.71308227e-01, -3.06765401e-02,\n",
            "        3.58566293e-01,  2.25142184e-01,  4.21790914e-01, -2.28038040e-01,\n",
            "       -3.18134700e-01,  7.97812669e-02,  1.52724309e+00, -7.82424459e-02,\n",
            "        3.49363600e-02,  1.49700000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.0680e+04, -1.3213e+00,  7.3616e-01,  5.5214e-01, -8.6029e-01,\n",
            "         -2.7634e-01, -1.1207e+00,  1.5707e+00, -2.9395e-01, -6.4630e-01,\n",
            "         -6.7993e-01, -4.1936e-01, -3.1017e-02,  1.1040e-01,  3.0061e-01,\n",
            "          2.4726e-01,  1.7943e-01, -3.6271e-01, -6.3310e-01, -3.9801e-01,\n",
            "         -2.4025e-01, -4.6056e-02, -1.1631e-01, -3.2484e-01,  4.8765e-01,\n",
            "          6.1904e-01,  9.4710e-01, -6.0206e-01, -4.0620e-01,  1.5495e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.0681e+04,  1.0228e+00, -7.6581e-01,  8.1483e-01, -2.7707e-01,\n",
            "         -1.0587e+00, -6.6452e-02, -6.5822e-01,  6.6483e-04,  7.8613e-01,\n",
            "         -3.2925e-01, -9.0077e-01,  3.7160e-01,  1.4106e+00, -4.8868e-01,\n",
            "          1.6597e+00,  1.1261e+00, -9.2214e-01,  3.7131e-01, -3.0677e-02,\n",
            "          3.5857e-01,  2.2514e-01,  4.2179e-01, -2.2804e-01, -3.1813e-01,\n",
            "          7.9781e-02,  1.5272e+00, -7.8242e-02,  3.4936e-02,  1.4970e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73536\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73537\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73538\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73539\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73540\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73541\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73542\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73543\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73544\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73545\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73546\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73547\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73548\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73549\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73550\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73551\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73552\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73553\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73554\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73555\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73556\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73557\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73558\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73559\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73560\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73561\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73562\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73563\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73564\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73565\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73566\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73567\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73568\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73569\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73570\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73571\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73572\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73573\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73574\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73575\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73576\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73577\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73578\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73579\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73580\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73581\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73582\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73583\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73584\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73585\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73586\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73587\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73588\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73589\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73590\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73591\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73592\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73593\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73594\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73595\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73596\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73597\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73598\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73599\n",
            "Chosing  random actions for the batch :  [0]\n",
            "Experiences :  [Experience(state=array([ 3.48980000e+04, -1.63562410e+00,  7.34515534e-02,  1.43126044e+00,\n",
            "       -8.72233536e-01, -1.13061632e+00, -2.91294352e-01,  9.76436574e-01,\n",
            "        4.81011330e-01, -8.03660542e-01, -1.19760610e+00,  1.02095221e+00,\n",
            "        3.54172996e-01, -5.85059031e-01,  6.62986905e-01,  5.19241479e-02,\n",
            "        1.02431089e+00, -6.53378464e-01,  3.02492080e-01, -1.98452900e-01,\n",
            "        4.45591469e-01,  6.16587646e-02, -6.71496614e-01,  5.30887705e-01,\n",
            "        3.09084969e-01,  8.00728435e-02,  6.46309751e-01, -2.70798774e-01,\n",
            "       -2.65699019e-02,  2.70000000e+02]), action=tensor([0]), reward=1, next_state=array([ 3.48980000e+04,  1.18284184e+00, -1.65057557e-01,  6.66152025e-01,\n",
            "       -1.74435064e-01, -7.02731213e-01, -3.61431232e-01, -4.29809388e-01,\n",
            "        1.44967196e-01,  2.18389937e-01, -6.87248291e-02,  1.76223086e+00,\n",
            "        7.11302245e-01, -6.73829074e-01,  4.98890729e-01,  8.61153891e-01,\n",
            "        2.81873072e-01, -2.24458228e-01, -3.57425722e-01, -1.12472848e-01,\n",
            "       -1.35478507e-01, -4.90611278e-02, -1.63063458e-01,  1.47597749e-01,\n",
            "        2.68633779e-01, -3.48182053e-02,  9.06256823e-01, -6.21101943e-02,\n",
            "       -3.53199870e-03,  2.00000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.4898e+04, -1.6356e+00,  7.3452e-02,  1.4313e+00, -8.7223e-01,\n",
            "         -1.1306e+00, -2.9129e-01,  9.7644e-01,  4.8101e-01, -8.0366e-01,\n",
            "         -1.1976e+00,  1.0210e+00,  3.5417e-01, -5.8506e-01,  6.6299e-01,\n",
            "          5.1924e-02,  1.0243e+00, -6.5338e-01,  3.0249e-01, -1.9845e-01,\n",
            "          4.4559e-01,  6.1659e-02, -6.7150e-01,  5.3089e-01,  3.0908e-01,\n",
            "          8.0073e-02,  6.4631e-01, -2.7080e-01, -2.6570e-02,  2.7000e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.4898e+04,  1.1828e+00, -1.6506e-01,  6.6615e-01, -1.7444e-01,\n",
            "         -7.0273e-01, -3.6143e-01, -4.2981e-01,  1.4497e-01,  2.1839e-01,\n",
            "         -6.8725e-02,  1.7622e+00,  7.1130e-01, -6.7383e-01,  4.9889e-01,\n",
            "          8.6115e-01,  2.8187e-01, -2.2446e-01, -3.5743e-01, -1.1247e-01,\n",
            "         -1.3548e-01, -4.9061e-02, -1.6306e-01,  1.4760e-01,  2.6863e-01,\n",
            "         -3.4818e-02,  9.0626e-01, -6.2110e-02, -3.5320e-03,  2.0000e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73600\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73601\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73602\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73603\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73604\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73605\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73606\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73607\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73608\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73609\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73610\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73611\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73612\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73613\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73614\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73615\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73616\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73617\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73618\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73619\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73620\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73621\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73622\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73623\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73624\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73625\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73626\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73627\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73628\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73629\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73630\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73631\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73632\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73633\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73634\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73635\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73636\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73637\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73638\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73639\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73640\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73641\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73642\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73643\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73644\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73645\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73646\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73647\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73648\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73649\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73650\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73651\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73652\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73653\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73654\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73655\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73656\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73657\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73658\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73659\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73660\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73661\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73662\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73663\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 2.57350000e+04, -4.78733149e-01,  1.44493795e+00,  4.03372540e-01,\n",
            "        7.69303073e-01,  1.13330099e+00, -5.07179881e-01,  1.31549259e+00,\n",
            "       -5.93619801e-01, -6.08453774e-01, -1.60599267e-01, -5.87620664e-01,\n",
            "        8.01182525e-03,  1.38254603e+00, -1.32307256e+00,  9.58663689e-01,\n",
            "       -2.80740442e-02,  2.33156824e-01,  5.06374254e-01,  6.80633808e-01,\n",
            "        1.30544571e-01, -1.18655549e-02,  1.62303904e-01, -2.67571230e-01,\n",
            "       -5.15516382e-01, -1.04449847e-01, -3.81333965e-01, -3.94209728e-01,\n",
            "       -7.31440431e-03,  1.51800000e+01]), action=tensor([0]), reward=1, next_state=array([ 2.57370000e+04, -8.19103895e+00, -4.72564799e+00,  2.13054394e-01,\n",
            "       -1.10717411e-01,  2.16710182e+00, -1.51447982e+00,  1.15980363e+00,\n",
            "       -1.49896260e+00,  2.56991977e+00,  3.07758376e+00,  2.23727618e+00,\n",
            "        2.15107133e-01, -3.30028938e-02, -1.68129663e+00,  1.17808558e+00,\n",
            "        1.02569146e+00, -1.51449322e+00, -8.28923325e-01,  2.80866857e-01,\n",
            "       -5.78127412e+00, -2.55673302e+00,  9.92230628e-01,  3.85193571e+00,\n",
            "        2.30646527e-01,  1.06611040e+00,  1.04992824e+00,  2.29945936e+00,\n",
            "       -1.47883152e+00,  8.00000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 2.5735e+04, -4.7873e-01,  1.4449e+00,  4.0337e-01,  7.6930e-01,\n",
            "          1.1333e+00, -5.0718e-01,  1.3155e+00, -5.9362e-01, -6.0845e-01,\n",
            "         -1.6060e-01, -5.8762e-01,  8.0118e-03,  1.3825e+00, -1.3231e+00,\n",
            "          9.5866e-01, -2.8074e-02,  2.3316e-01,  5.0637e-01,  6.8063e-01,\n",
            "          1.3054e-01, -1.1866e-02,  1.6230e-01, -2.6757e-01, -5.1552e-01,\n",
            "         -1.0445e-01, -3.8133e-01, -3.9421e-01, -7.3144e-03,  1.5180e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 2.5737e+04, -8.1910e+00, -4.7256e+00,  2.1305e-01, -1.1072e-01,\n",
            "          2.1671e+00, -1.5145e+00,  1.1598e+00, -1.4990e+00,  2.5699e+00,\n",
            "          3.0776e+00,  2.2373e+00,  2.1511e-01, -3.3003e-02, -1.6813e+00,\n",
            "          1.1781e+00,  1.0257e+00, -1.5145e+00, -8.2892e-01,  2.8087e-01,\n",
            "         -5.7813e+00, -2.5567e+00,  9.9223e-01,  3.8519e+00,  2.3065e-01,\n",
            "          1.0661e+00,  1.0499e+00,  2.2995e+00, -1.4788e+00,  8.0000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-98.9972, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73664\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73665\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73666\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73667\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73668\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73669\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73670\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73671\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73672\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73673\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73674\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73675\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73676\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73677\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73678\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73679\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73680\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73681\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73682\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73683\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73684\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73685\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73686\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73687\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73688\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73689\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73690\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73691\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73692\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73693\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73694\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73695\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73696\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73697\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73698\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73699\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73700\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73701\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73702\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73703\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73704\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73705\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73706\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73707\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73708\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73709\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73710\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73711\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73712\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73713\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73714\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73715\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73716\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73717\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73718\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73719\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73720\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73721\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73722\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73723\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73724\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73725\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73726\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73727\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 4.14410000e+04,  1.21547630e+00, -1.11707078e-01,  7.78309370e-01,\n",
            "        8.84500141e-01, -4.13460340e-01,  6.63572455e-01, -7.40023896e-01,\n",
            "        3.41586660e-01,  7.22396218e-01,  5.26179394e-02, -1.64162152e-01,\n",
            "        2.01338263e-01, -7.18189221e-01,  6.72701011e-02,  1.09397219e-01,\n",
            "        8.51121445e-01, -1.02723545e+00,  1.06107355e+00,  3.73595744e-01,\n",
            "       -1.53170561e-01, -4.60728043e-02, -4.24422343e-02, -1.44297605e-01,\n",
            "       -8.69211767e-01,  4.86813211e-01, -3.40993612e-01,  5.91857034e-02,\n",
            "        1.36802218e-02,  1.00000000e+00]), action=tensor([0]), reward=1, next_state=array([ 4.14420000e+04, -7.12744824e-01,  1.01997011e+00,  1.92439018e+00,\n",
            "       -8.05740758e-02,  7.14554943e-01, -3.78487453e-01,  1.24060029e+00,\n",
            "       -2.51638694e-01, -6.45090869e-01, -6.16833920e-01, -2.04515001e-01,\n",
            "        1.32238466e-01,  3.53502272e-01, -4.28717028e-02,  5.79624377e-01,\n",
            "       -2.14452043e-01, -4.85495212e-01, -5.93523534e-01, -1.19210574e+00,\n",
            "       -2.95574660e-02,  6.73410240e-02,  3.52712717e-01, -4.08705080e-01,\n",
            "        9.03989508e-02,  6.15330348e-01, -4.08992803e-01, -1.16133830e-01,\n",
            "       -1.62332276e-01,  1.30000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 4.1441e+04,  1.2155e+00, -1.1171e-01,  7.7831e-01,  8.8450e-01,\n",
            "         -4.1346e-01,  6.6357e-01, -7.4002e-01,  3.4159e-01,  7.2240e-01,\n",
            "          5.2618e-02, -1.6416e-01,  2.0134e-01, -7.1819e-01,  6.7270e-02,\n",
            "          1.0940e-01,  8.5112e-01, -1.0272e+00,  1.0611e+00,  3.7360e-01,\n",
            "         -1.5317e-01, -4.6073e-02, -4.2442e-02, -1.4430e-01, -8.6921e-01,\n",
            "          4.8681e-01, -3.4099e-01,  5.9186e-02,  1.3680e-02,  1.0000e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 4.1442e+04, -7.1274e-01,  1.0200e+00,  1.9244e+00, -8.0574e-02,\n",
            "          7.1455e-01, -3.7849e-01,  1.2406e+00, -2.5164e-01, -6.4509e-01,\n",
            "         -6.1683e-01, -2.0452e-01,  1.3224e-01,  3.5350e-01, -4.2872e-02,\n",
            "          5.7962e-01, -2.1445e-01, -4.8550e-01, -5.9352e-01, -1.1921e+00,\n",
            "         -2.9557e-02,  6.7341e-02,  3.5271e-01, -4.0871e-01,  9.0399e-02,\n",
            "          6.1533e-01, -4.0899e-01, -1.1613e-01, -1.6233e-01,  1.3000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73728\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73729\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73730\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73731\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73732\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73733\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73734\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73735\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73736\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73737\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73738\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73739\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73740\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73741\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73742\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73743\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73744\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73745\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73746\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73747\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73748\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73749\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73750\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73751\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73752\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73753\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73754\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73755\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73756\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73757\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73758\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73759\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73760\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73761\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73762\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73763\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73764\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73765\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73766\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73767\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73768\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73769\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73770\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73771\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73772\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73773\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73774\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73775\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73776\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73777\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73778\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73779\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73780\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73781\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73782\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73783\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73784\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73785\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73786\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73787\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73788\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73789\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73790\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73791\n",
            "Chosing  random actions for the batch :  [0]\n",
            "Experiences :  [Experience(state=array([ 5.39100000e+04, -1.47013894e+00,  8.26562424e-01,  1.34529329e+00,\n",
            "       -1.55559722e+00, -2.72703123e-03, -5.02816925e-01,  8.43835271e-01,\n",
            "       -2.27756319e-01,  3.90667501e-01,  4.79252527e-01,  1.14429681e+00,\n",
            "        6.21902313e-01,  2.79629099e-01, -3.72116009e-01, -8.94647179e-02,\n",
            "        7.70848376e-01, -1.21393744e+00,  1.33153889e-01, -3.11243431e-01,\n",
            "        2.41675043e-01, -1.80356962e-01, -1.36755942e-01, -1.73377171e-01,\n",
            "        6.28335385e-02, -2.60143012e-01,  6.79041504e-01,  5.39440350e-02,\n",
            "        1.66227388e-01,  6.50000000e+01]), action=tensor([0]), reward=1, next_state=array([ 5.39100000e+04, -1.04614876e+00,  7.85744706e-01,  5.66596213e-01,\n",
            "       -1.90865215e-01, -8.66683999e-01,  8.45066679e-01,  1.17050893e+00,\n",
            "        5.05273092e-01, -8.92393116e-01, -6.87864981e-01,  1.49881076e+00,\n",
            "        6.91681979e-01, -3.64370519e-01,  8.09174586e-01,  5.19896727e-01,\n",
            "       -8.11364722e-02,  1.01584684e-01, -6.51668993e-01, -3.73803859e-01,\n",
            "       -2.12411263e-01,  6.77185335e-02,  6.23700568e-02,  3.18826577e-01,\n",
            "       -2.88362189e-01, -7.41360731e-01,  2.03387993e-01, -3.30471245e-02,\n",
            "        1.17459842e-02,  2.46700000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.3910e+04, -1.4701e+00,  8.2656e-01,  1.3453e+00, -1.5556e+00,\n",
            "         -2.7270e-03, -5.0282e-01,  8.4384e-01, -2.2776e-01,  3.9067e-01,\n",
            "          4.7925e-01,  1.1443e+00,  6.2190e-01,  2.7963e-01, -3.7212e-01,\n",
            "         -8.9465e-02,  7.7085e-01, -1.2139e+00,  1.3315e-01, -3.1124e-01,\n",
            "          2.4168e-01, -1.8036e-01, -1.3676e-01, -1.7338e-01,  6.2834e-02,\n",
            "         -2.6014e-01,  6.7904e-01,  5.3944e-02,  1.6623e-01,  6.5000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.3910e+04, -1.0461e+00,  7.8574e-01,  5.6660e-01, -1.9087e-01,\n",
            "         -8.6668e-01,  8.4507e-01,  1.1705e+00,  5.0527e-01, -8.9239e-01,\n",
            "         -6.8786e-01,  1.4988e+00,  6.9168e-01, -3.6437e-01,  8.0917e-01,\n",
            "          5.1990e-01, -8.1136e-02,  1.0158e-01, -6.5167e-01, -3.7380e-01,\n",
            "         -2.1241e-01,  6.7719e-02,  6.2370e-02,  3.1883e-01, -2.8836e-01,\n",
            "         -7.4136e-01,  2.0339e-01, -3.3047e-02,  1.1746e-02,  2.4670e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73792\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73793\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73794\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73795\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73796\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73797\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73798\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73799\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73800\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73801\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73802\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73803\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73804\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73805\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73806\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73807\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73808\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73809\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73810\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73811\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73812\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73813\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73814\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73815\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73816\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73817\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73818\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73819\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73820\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73821\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73822\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73823\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73824\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73825\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73826\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73827\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73828\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73829\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73830\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73831\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73832\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73833\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73834\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73835\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73836\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73837\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73838\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73839\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73840\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73841\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73842\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73843\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73844\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73845\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73846\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73847\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73848\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73849\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73850\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73851\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73852\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73853\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73854\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73855\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 4.10720000e+04, -3.95612450e+00, -2.46674664e+00,  3.41482776e-01,\n",
            "       -1.08060672e+00, -1.76706427e+00, -8.83044333e-02,  6.79967298e-01,\n",
            "        7.27664082e-01,  6.15285616e-01, -2.37648701e+00,  5.53038899e-01,\n",
            "        1.51531351e+00,  5.86349956e-01,  4.63627935e-01,  1.36867839e-01,\n",
            "        5.00030639e-01, -4.89741766e-01,  1.16417973e+00,  2.69625139e-01,\n",
            "        2.54466117e-01,  4.14557014e-01,  5.01062233e-01, -5.70391800e-01,\n",
            "        1.11512457e-01,  6.42408532e-01, -6.56156112e-01,  5.13213037e-02,\n",
            "       -6.29273411e-01,  5.07960000e+02]), action=[0], reward=0, next_state=array([ 4.10720000e+04, -1.07783339e-01, -4.82670730e-01,  1.73585488e+00,\n",
            "       -1.41264333e+00, -1.27962723e+00, -1.46062713e-01, -5.07826099e-01,\n",
            "        4.89487562e-02, -2.41224074e+00,  1.16784427e+00, -3.70116548e-01,\n",
            "       -1.34229433e+00,  1.35279631e-01, -2.89091969e-01,  1.35387927e+00,\n",
            "       -9.45639928e-01,  1.09042833e+00, -1.63380811e-01,  5.19347148e-01,\n",
            "       -9.17835039e-02, -2.01820126e-01, -2.82120839e-01,  2.37784468e-01,\n",
            "        1.96540513e-02, -7.48219063e-01, -3.08983624e-01,  1.86780033e-01,\n",
            "        1.55882933e-01,  5.50000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 4.1072e+04, -3.9561e+00, -2.4667e+00,  3.4148e-01, -1.0806e+00,\n",
            "         -1.7671e+00, -8.8304e-02,  6.7997e-01,  7.2766e-01,  6.1529e-01,\n",
            "         -2.3765e+00,  5.5304e-01,  1.5153e+00,  5.8635e-01,  4.6363e-01,\n",
            "          1.3687e-01,  5.0003e-01, -4.8974e-01,  1.1642e+00,  2.6963e-01,\n",
            "          2.5447e-01,  4.1456e-01,  5.0106e-01, -5.7039e-01,  1.1151e-01,\n",
            "          6.4241e-01, -6.5616e-01,  5.1321e-02, -6.2927e-01,  5.0796e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[ 4.1072e+04, -1.0778e-01, -4.8267e-01,  1.7359e+00, -1.4126e+00,\n",
            "         -1.2796e+00, -1.4606e-01, -5.0783e-01,  4.8949e-02, -2.4122e+00,\n",
            "          1.1678e+00, -3.7012e-01, -1.3423e+00,  1.3528e-01, -2.8909e-01,\n",
            "          1.3539e+00, -9.4564e-01,  1.0904e+00, -1.6338e-01,  5.1935e-01,\n",
            "         -9.1784e-02, -2.0182e-01, -2.8212e-01,  2.3778e-01,  1.9654e-02,\n",
            "         -7.4822e-01, -3.0898e-01,  1.8678e-01,  1.5588e-01,  5.5000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(1.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73856\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73857\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73858\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73859\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73860\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73861\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73862\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73863\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73864\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73865\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73866\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73867\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73868\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73869\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73870\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73871\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73872\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73873\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73874\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73875\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73876\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73877\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73878\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73879\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73880\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73881\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73882\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73883\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73884\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73885\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73886\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73887\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73888\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73889\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73890\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73891\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73892\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73893\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73894\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73895\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73896\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73897\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73898\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73899\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73900\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73901\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73902\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73903\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73904\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73905\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73906\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73907\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73908\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73909\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73910\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73911\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73912\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73913\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73914\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73915\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73916\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73917\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73918\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73919\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.88610000e+04, -3.11538565e-01,  1.37920254e+00,  1.15447809e+00,\n",
            "        1.84214670e+00,  8.68874588e-01, -2.69177536e-01,  1.11037656e+00,\n",
            "       -7.04573335e-02, -1.64023627e+00,  2.63211551e-01, -4.34073473e-01,\n",
            "       -2.40045613e-01,  5.85277708e-02,  3.97483034e-01,  2.26501117e-01,\n",
            "       -6.28806490e-02, -1.94574495e-01, -8.08729856e-01, -1.10589080e+00,\n",
            "       -1.30099566e-01,  8.75850173e-02,  2.30099205e-01, -2.27054846e-01,\n",
            "        8.37795973e-02,  2.24885827e-01,  5.70517643e-02,  3.54601484e-03,\n",
            "        1.31520491e-02,  1.56000000e+00]), action=tensor([0]), reward=1, next_state=array([ 3.88610000e+04, -7.83294263e-01,  1.55901674e+00,  1.11287076e+00,\n",
            "        1.78925520e+00,  6.28174528e-01, -6.84408215e-01,  1.32695200e+00,\n",
            "       -5.62092533e-01, -9.92009036e-01,  1.09577969e+00, -7.00431606e-01,\n",
            "       -9.76053658e-01, -9.75857512e-01,  1.86852095e-01,  1.59431962e-01,\n",
            "        1.37159404e-01, -5.53387453e-01, -2.98328062e-01, -7.22109745e-01,\n",
            "       -1.74702128e-01,  8.11670861e-02,  5.25141646e-02, -2.45252368e-01,\n",
            "        3.63440758e-01,  1.81630062e-01, -6.29830190e-02, -8.86736194e-01,\n",
            "       -1.65917631e-01,  1.05400000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.8861e+04, -3.1154e-01,  1.3792e+00,  1.1545e+00,  1.8421e+00,\n",
            "          8.6887e-01, -2.6918e-01,  1.1104e+00, -7.0457e-02, -1.6402e+00,\n",
            "          2.6321e-01, -4.3407e-01, -2.4005e-01,  5.8528e-02,  3.9748e-01,\n",
            "          2.2650e-01, -6.2881e-02, -1.9457e-01, -8.0873e-01, -1.1059e+00,\n",
            "         -1.3010e-01,  8.7585e-02,  2.3010e-01, -2.2705e-01,  8.3780e-02,\n",
            "          2.2489e-01,  5.7052e-02,  3.5460e-03,  1.3152e-02,  1.5600e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.8861e+04, -7.8329e-01,  1.5590e+00,  1.1129e+00,  1.7893e+00,\n",
            "          6.2817e-01, -6.8441e-01,  1.3270e+00, -5.6209e-01, -9.9201e-01,\n",
            "          1.0958e+00, -7.0043e-01, -9.7605e-01, -9.7586e-01,  1.8685e-01,\n",
            "          1.5943e-01,  1.3716e-01, -5.5339e-01, -2.9833e-01, -7.2211e-01,\n",
            "         -1.7470e-01,  8.1167e-02,  5.2514e-02, -2.4525e-01,  3.6344e-01,\n",
            "          1.8163e-01, -6.2983e-02, -8.8674e-01, -1.6592e-01,  1.0540e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[0.9994]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-98.9377, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73920\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73921\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73922\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73923\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73924\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73925\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73926\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73927\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73928\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73929\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73930\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73931\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73932\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73933\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73934\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73935\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73936\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73937\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73938\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73939\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73940\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73941\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73942\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73943\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73944\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73945\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73946\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73947\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73948\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73949\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73950\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73951\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73952\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73953\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73954\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73955\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73956\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73957\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73958\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73959\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73960\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73961\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73962\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73963\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73964\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73965\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73966\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73967\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73968\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73969\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73970\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73971\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73972\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73973\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73974\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73975\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73976\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73977\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73978\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73979\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  73980\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73981\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73982\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73983\n",
            "Chosing  random actions for the batch :  [1]\n",
            "Experiences :  [Experience(state=array([ 8.82700000e+03, -1.74649408e+00, -2.21827662e-02,  1.70097810e+00,\n",
            "       -3.49482711e+00, -5.38016584e-01, -8.00347901e-01, -1.06466889e-01,\n",
            "        5.17636606e-01,  2.78629541e+00, -2.99937471e+00,  1.68985763e+00,\n",
            "       -6.65989060e-01,  2.24097890e+00,  1.40180710e+00, -1.01424289e+00,\n",
            "        2.64555226e-02, -1.13500795e-01,  1.23803204e+00, -3.44717344e-01,\n",
            "        1.20070328e-01,  1.52428583e-01,  5.71972984e-01, -3.34028014e-01,\n",
            "       -1.97824310e-02,  9.49106685e-01, -9.07445263e-01,  1.67863012e-01,\n",
            "       -2.06054851e-02,  7.41100000e+01]), action=tensor([1]), reward=-1, next_state=array([ 8.83100000e+03,  1.25653571e+00,  4.89930462e-01,  4.52273686e-01,\n",
            "        8.93401509e-01, -2.60968301e-01, -1.01306306e+00,  2.32613736e-02,\n",
            "       -3.07098645e-01,  1.25741485e+00, -6.65752202e-01,  9.44472386e-01,\n",
            "       -2.06221340e+00,  2.26452804e+00,  1.06812039e+00,  3.13490913e-01,\n",
            "        4.32429981e-01,  7.02431219e-01, -7.24620693e-02, -4.02870263e-01,\n",
            "       -9.47052483e-02, -4.32796331e-01, -9.85540552e-01,  1.34097153e-01,\n",
            "        2.89764085e-01,  2.06480812e-01,  5.46235065e-02, -4.75547242e-02,\n",
            "        2.55231440e-02,  3.59000000e+00]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 8.8270e+03, -1.7465e+00, -2.2183e-02,  1.7010e+00, -3.4948e+00,\n",
            "         -5.3802e-01, -8.0035e-01, -1.0647e-01,  5.1764e-01,  2.7863e+00,\n",
            "         -2.9994e+00,  1.6899e+00, -6.6599e-01,  2.2410e+00,  1.4018e+00,\n",
            "         -1.0142e+00,  2.6456e-02, -1.1350e-01,  1.2380e+00, -3.4472e-01,\n",
            "          1.2007e-01,  1.5243e-01,  5.7197e-01, -3.3403e-01, -1.9782e-02,\n",
            "          9.4911e-01, -9.0745e-01,  1.6786e-01, -2.0605e-02,  7.4110e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[-1.]], device='cuda:0', dtype=torch.float64), tensor([[ 8.8310e+03,  1.2565e+00,  4.8993e-01,  4.5227e-01,  8.9340e-01,\n",
            "         -2.6097e-01, -1.0131e+00,  2.3261e-02, -3.0710e-01,  1.2574e+00,\n",
            "         -6.6575e-01,  9.4447e-01, -2.0622e+00,  2.2645e+00,  1.0681e+00,\n",
            "          3.1349e-01,  4.3243e-01,  7.0243e-01, -7.2462e-02, -4.0287e-01,\n",
            "         -9.4705e-02, -4.3280e-01, -9.8554e-01,  1.3410e-01,  2.8976e-01,\n",
            "          2.0648e-01,  5.4624e-02, -4.7555e-02,  2.5523e-02,  3.5900e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[0.9999]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-0.4018, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  73984\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73985\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73986\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73987\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73988\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73989\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73990\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73991\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  73992\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73993\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73994\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73995\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73996\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73997\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73998\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  73999\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74000\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74001\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74002\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74003\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74004\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74005\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74006\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74007\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74008\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74009\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74010\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74011\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74012\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74013\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74014\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74015\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74016\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74017\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74018\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74019\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74020\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74021\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74022\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74023\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74024\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74025\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74026\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74027\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74028\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74029\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74030\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74031\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74032\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74033\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74034\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74035\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74036\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74037\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74038\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74039\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74040\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74041\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74042\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74043\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74044\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74045\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74046\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74047\n",
            "Chosing  random actions for the batch :  [0]\n",
            "Experiences :  [Experience(state=array([ 4.87700000e+03, -6.42694937e-01,  1.37968033e+00,  1.51491188e+00,\n",
            "       -9.24676355e-02,  2.80138366e-01, -3.20636717e-01,  5.81982109e-01,\n",
            "       -4.01686242e-02,  6.50602150e-01, -2.42805397e-01,  2.19189347e+00,\n",
            "       -1.29621837e+00,  2.90487632e+00,  1.43174540e+00, -6.90281340e-01,\n",
            "        3.80641338e-01, -1.11656820e-01,  3.51725211e-01,  1.85034625e-01,\n",
            "        2.79899945e-01, -3.64975687e-01, -6.46007637e-01, -6.72143783e-02,\n",
            "       -4.64997914e-02, -1.09471139e-01,  3.59964814e-02,  3.49719185e-01,\n",
            "        1.61963210e-01,  1.78000000e+00]), action=tensor([1]), reward=-1, next_state=array([ 4.88000000e+03, -1.51735064e+00,  1.46106779e+00,  1.11708930e+00,\n",
            "        1.06415202e+00, -2.17402828e-01, -9.55470377e-01,  6.39495070e-01,\n",
            "       -4.26124398e-01,  1.61184495e+00,  6.47664837e-01,  5.39372937e-01,\n",
            "       -3.45387670e+00, -6.02603360e-02,  1.73299042e+00,  5.46267232e-01,\n",
            "       -4.82098001e-01,  6.57007637e-01,  3.99027861e-01,  1.34581867e-01,\n",
            "       -2.50221320e-01, -3.55852407e-02,  2.77689669e-01,  2.55565282e-02,\n",
            "        6.17705102e-01, -3.89384898e-01, -5.07863948e-01, -1.02585872e+00,\n",
            "       -1.93131964e-01,  1.56200000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 4.8770e+03, -6.4269e-01,  1.3797e+00,  1.5149e+00, -9.2468e-02,\n",
            "          2.8014e-01, -3.2064e-01,  5.8198e-01, -4.0169e-02,  6.5060e-01,\n",
            "         -2.4281e-01,  2.1919e+00, -1.2962e+00,  2.9049e+00,  1.4317e+00,\n",
            "         -6.9028e-01,  3.8064e-01, -1.1166e-01,  3.5173e-01,  1.8503e-01,\n",
            "          2.7990e-01, -3.6498e-01, -6.4601e-01, -6.7214e-02, -4.6500e-02,\n",
            "         -1.0947e-01,  3.5996e-02,  3.4972e-01,  1.6196e-01,  1.7800e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[-1.]], device='cuda:0', dtype=torch.float64), tensor([[ 4.8800e+03, -1.5174e+00,  1.4611e+00,  1.1171e+00,  1.0642e+00,\n",
            "         -2.1740e-01, -9.5547e-01,  6.3950e-01, -4.2612e-01,  1.6118e+00,\n",
            "          6.4766e-01,  5.3937e-01, -3.4539e+00, -6.0260e-02,  1.7330e+00,\n",
            "          5.4627e-01, -4.8210e-01,  6.5701e-01,  3.9903e-01,  1.3458e-01,\n",
            "         -2.5022e-01, -3.5585e-02,  2.7769e-01,  2.5557e-02,  6.1771e-01,\n",
            "         -3.8938e-01, -5.0786e-01, -1.0259e+00, -1.9313e-01,  1.5620e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[0.9905]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-0.6203, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74048\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74049\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74050\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74051\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74052\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74053\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74054\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74055\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74056\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74057\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74058\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74059\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74060\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74061\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74062\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74063\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74064\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74065\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74066\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74067\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74068\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74069\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74070\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74071\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74072\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74073\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74074\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74075\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74076\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74077\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74078\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74079\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74080\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74081\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74082\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74083\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74084\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74085\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74086\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74087\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74088\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74089\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74090\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74091\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74092\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74093\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74094\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74095\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74096\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74097\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74098\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74099\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74100\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74101\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74102\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74103\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74104\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74105\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74106\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74107\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74108\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74109\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74110\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74111\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.21230000e+04, -1.35830828e+01,  7.93539015e+00, -1.34445639e+01,\n",
            "        2.27653124e+00, -8.38361876e+00, -2.83618069e+00, -5.69718020e+00,\n",
            "        8.99672218e+00,  9.01056619e-02,  9.34393892e-02, -2.00743236e+00,\n",
            "        2.91005485e+00,  1.01755869e+00,  2.11799861e+00,  8.98081181e-01,\n",
            "        3.52685110e+00,  6.31109277e+00,  2.01070908e+00, -1.25984244e+00,\n",
            "        1.35446618e-01, -2.59166814e-02, -1.32718471e+00, -4.52368623e-01,\n",
            "        9.44348075e-02,  7.25668905e-01, -3.54562323e-01, -2.02308791e-01,\n",
            "       -3.43580337e-01,  8.99900000e+01]), action=tensor([0]), reward=1, next_state=array([ 3.21240000e+04, -1.82628208e+00, -4.01447979e-01,  1.49955159e+00,\n",
            "       -1.09296258e-01,  1.28026407e+00, -1.81313613e+00,  1.96236344e-01,\n",
            "       -1.07236525e-01, -3.12582040e-01, -7.96411309e-01, -3.46424101e-01,\n",
            "       -1.16482369e-01, -1.15932604e-01, -3.92268280e-01,  5.07645022e-01,\n",
            "        6.97707852e-01, -2.82745649e-01, -1.81034073e-01, -8.14060669e-01,\n",
            "       -4.60621551e-02, -1.80360851e-01, -7.94717579e-01,  2.74337133e-02,\n",
            "        3.39534124e-01, -5.15086671e-02, -7.40772055e-02, -1.16992451e-01,\n",
            "        2.69898248e-01,  4.99900000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.2123e+04, -1.3583e+01,  7.9354e+00, -1.3445e+01,  2.2765e+00,\n",
            "         -8.3836e+00, -2.8362e+00, -5.6972e+00,  8.9967e+00,  9.0106e-02,\n",
            "          9.3439e-02, -2.0074e+00,  2.9101e+00,  1.0176e+00,  2.1180e+00,\n",
            "          8.9808e-01,  3.5269e+00,  6.3111e+00,  2.0107e+00, -1.2598e+00,\n",
            "          1.3545e-01, -2.5917e-02, -1.3272e+00, -4.5237e-01,  9.4435e-02,\n",
            "          7.2567e-01, -3.5456e-01, -2.0231e-01, -3.4358e-01,  8.9990e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.2124e+04, -1.8263e+00, -4.0145e-01,  1.4996e+00, -1.0930e-01,\n",
            "          1.2803e+00, -1.8131e+00,  1.9624e-01, -1.0724e-01, -3.1258e-01,\n",
            "         -7.9641e-01, -3.4642e-01, -1.1648e-01, -1.1593e-01, -3.9227e-01,\n",
            "          5.0765e-01,  6.9771e-01, -2.8275e-01, -1.8103e-01, -8.1406e-01,\n",
            "         -4.6062e-02, -1.8036e-01, -7.9472e-01,  2.7434e-02,  3.3953e-01,\n",
            "         -5.1509e-02, -7.4077e-02, -1.1699e-01,  2.6990e-01,  4.9990e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74112\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74113\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74114\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74115\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74116\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74117\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74118\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74119\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74120\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74121\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74122\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74123\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74124\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74125\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74126\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74127\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74128\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74129\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74130\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74131\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74132\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74133\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74134\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74135\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74136\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74137\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74138\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74139\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74140\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74141\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74142\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74143\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74144\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74145\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74146\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74147\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74148\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74149\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74150\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74151\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74152\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74153\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74154\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74155\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74156\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74157\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74158\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74159\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74160\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74161\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74162\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74163\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74164\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74165\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74166\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74167\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74168\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74169\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74170\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74171\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74172\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74173\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74174\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74175\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 6.71400000e+03, -5.12242261e-01,  7.54218636e-01,  1.68524311e+00,\n",
            "        3.97027647e-01, -4.46044673e-02, -5.62870866e-02,  2.53859977e-01,\n",
            "        1.70461493e-01,  1.69586022e+00, -8.09455808e-01, -9.72632437e-02,\n",
            "       -3.17357412e+00, -2.47202433e-01,  1.50015253e+00, -9.54014369e-01,\n",
            "       -7.44574124e-01,  1.20390653e+00, -3.46781226e-01, -2.22779925e-01,\n",
            "       -1.11517151e-01, -1.61392149e-01,  7.21488470e-02, -1.22441730e-01,\n",
            "        3.30900558e-02, -2.89680340e-01,  3.59247953e-01,  3.36844076e-01,\n",
            "        1.87657923e-01,  5.17000000e+00]), action=tensor([1]), reward=-1, next_state=array([ 6.71500000e+03,  1.20374463e-01, -1.85103098e+00,  1.79263722e+00,\n",
            "        2.63295255e+00, -1.52863566e+00,  2.25568289e+00, -1.14108250e+00,\n",
            "        6.85179743e-01,  2.34079260e+00, -3.61786956e-01,  1.44788019e+00,\n",
            "       -1.45067334e+00,  9.61155540e-01,  5.52172317e-01, -2.64307149e+00,\n",
            "       -1.73470796e-01,  1.29018712e+00, -2.84154718e-01, -1.02302719e+00,\n",
            "        6.21599718e-01,  2.39812249e-01,  4.32054575e-01, -3.87534430e-01,\n",
            "       -2.20359329e-01, -1.09109404e-01,  1.14069846e+00, -5.98994466e-02,\n",
            "        7.63692809e-02,  4.38140000e+02]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 6.7140e+03, -5.1224e-01,  7.5422e-01,  1.6852e+00,  3.9703e-01,\n",
            "         -4.4604e-02, -5.6287e-02,  2.5386e-01,  1.7046e-01,  1.6959e+00,\n",
            "         -8.0946e-01, -9.7263e-02, -3.1736e+00, -2.4720e-01,  1.5002e+00,\n",
            "         -9.5401e-01, -7.4457e-01,  1.2039e+00, -3.4678e-01, -2.2278e-01,\n",
            "         -1.1152e-01, -1.6139e-01,  7.2149e-02, -1.2244e-01,  3.3090e-02,\n",
            "         -2.8968e-01,  3.5925e-01,  3.3684e-01,  1.8766e-01,  5.1700e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[-1.]], device='cuda:0', dtype=torch.float64), tensor([[ 6.7150e+03,  1.2037e-01, -1.8510e+00,  1.7926e+00,  2.6330e+00,\n",
            "         -1.5286e+00,  2.2557e+00, -1.1411e+00,  6.8518e-01,  2.3408e+00,\n",
            "         -3.6179e-01,  1.4479e+00, -1.4507e+00,  9.6116e-01,  5.5217e-01,\n",
            "         -2.6431e+00, -1.7347e-01,  1.2902e+00, -2.8415e-01, -1.0230e+00,\n",
            "          6.2160e-01,  2.3981e-01,  4.3205e-01, -3.8753e-01, -2.2036e-01,\n",
            "         -1.0911e-01,  1.1407e+00, -5.9899e-02,  7.6369e-02,  4.3814e+02]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-0.3188, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74176\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74177\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74178\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74179\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74180\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74181\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74182\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74183\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74184\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74185\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74186\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74187\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74188\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74189\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74190\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74191\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74192\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74193\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74194\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74195\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74196\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74197\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74198\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74199\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74200\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74201\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74202\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74203\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74204\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74205\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74206\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74207\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74208\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74209\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74210\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74211\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74212\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74213\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74214\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74215\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74216\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74217\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74218\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74219\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74220\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74221\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74222\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74223\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74224\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74225\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74226\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74227\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74228\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74229\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74230\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74231\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74232\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74233\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74234\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74235\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74236\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74237\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74238\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74239\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 5.30560000e+04, -6.92013678e-01,  4.69893491e-01,  2.78263401e+00,\n",
            "        3.61489466e-01, -7.54755269e-01,  3.81401080e-01, -1.49827565e-01,\n",
            "        3.24135921e-01, -3.70831028e-02, -5.51506920e-01, -3.57703338e-01,\n",
            "       -3.93826514e-01, -2.01646558e-01, -1.59607940e-01,  2.11382206e+00,\n",
            "        1.93586047e-01, -8.53669655e-02,  3.03870228e-01, -8.38062642e-02,\n",
            "        1.48836735e-01,  3.28332342e-01,  9.12918122e-01, -1.81580434e-01,\n",
            "        1.05664851e-01, -3.17671759e-02,  6.35733197e-01,  7.90475290e-02,\n",
            "        8.55891674e-02,  4.57000000e+01]), action=tensor([0]), reward=1, next_state=array([ 5.30570000e+04, -5.05143577e-01,  8.35780428e-01,  1.27674613e+00,\n",
            "        6.53915851e-01, -2.28685109e-01,  6.84460491e-01, -2.94623226e-01,\n",
            "       -1.54441412e+00, -3.44155531e-02,  5.58346159e-01,  2.82823624e-01,\n",
            "       -8.70841676e-01, -1.92279896e+00,  5.49999372e-01,  1.41871383e+00,\n",
            "        1.55419424e-01, -2.85788446e-01,  1.28338273e+00,  2.04258089e+00,\n",
            "       -2.51498514e-01,  1.56851882e+00, -2.01457290e-01,  2.00218820e-02,\n",
            "       -5.31001739e-01, -7.18983114e-01,  5.19787785e-01,  1.06314903e-01,\n",
            "       -7.88361709e-02,  3.99000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 5.3056e+04, -6.9201e-01,  4.6989e-01,  2.7826e+00,  3.6149e-01,\n",
            "         -7.5476e-01,  3.8140e-01, -1.4983e-01,  3.2414e-01, -3.7083e-02,\n",
            "         -5.5151e-01, -3.5770e-01, -3.9383e-01, -2.0165e-01, -1.5961e-01,\n",
            "          2.1138e+00,  1.9359e-01, -8.5367e-02,  3.0387e-01, -8.3806e-02,\n",
            "          1.4884e-01,  3.2833e-01,  9.1292e-01, -1.8158e-01,  1.0566e-01,\n",
            "         -3.1767e-02,  6.3573e-01,  7.9048e-02,  8.5589e-02,  4.5700e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 5.3057e+04, -5.0514e-01,  8.3578e-01,  1.2767e+00,  6.5392e-01,\n",
            "         -2.2869e-01,  6.8446e-01, -2.9462e-01, -1.5444e+00, -3.4416e-02,\n",
            "          5.5835e-01,  2.8282e-01, -8.7084e-01, -1.9228e+00,  5.5000e-01,\n",
            "          1.4187e+00,  1.5542e-01, -2.8579e-01,  1.2834e+00,  2.0426e+00,\n",
            "         -2.5150e-01,  1.5685e+00, -2.0146e-01,  2.0022e-02, -5.3100e-01,\n",
            "         -7.1898e-01,  5.1979e-01,  1.0631e-01, -7.8836e-02,  3.9900e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74240\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74241\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74242\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74243\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74244\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74245\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74246\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74247\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74248\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74249\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74250\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74251\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74252\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74253\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74254\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74255\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74256\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74257\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74258\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74259\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74260\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74261\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74262\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74263\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74264\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74265\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74266\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74267\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74268\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74269\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74270\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74271\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74272\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74273\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74274\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74275\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74276\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74277\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74278\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74279\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74280\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74281\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74282\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74283\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74284\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74285\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74286\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74287\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74288\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74289\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74290\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74291\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74292\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74293\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74294\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74295\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74296\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74297\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74298\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74299\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74300\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74301\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74302\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74303\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.80770000e+04, -1.48267879e+00,  1.39679527e+00,  1.79835413e+00,\n",
            "        1.24939734e+00, -3.26993670e-01,  2.63220314e-01, -2.37322161e-02,\n",
            "        5.48947597e-01, -2.50385343e-01, -1.65310546e-01, -5.76943521e-01,\n",
            "        5.55276671e-01,  3.82335367e-01, -3.40996361e-02,  4.97901916e-01,\n",
            "       -8.47767642e-01,  6.88569570e-01, -7.61158376e-01,  8.19062921e-01,\n",
            "       -6.16094325e-03, -2.26077785e-01, -5.45663763e-01,  4.24153817e-02,\n",
            "        6.07964277e-02,  1.72936719e-01, -4.28513810e-01, -1.47092763e-01,\n",
            "        9.74589712e-02,  1.00000000e+00]), action=[0], reward=0, next_state=array([ 3.80790000e+04, -4.33142463e-01,  4.21165174e-01,  1.90300073e+00,\n",
            "       -5.45997469e-01, -6.79364046e-01, -5.59847590e-01,  3.44480912e-01,\n",
            "        1.50870281e-01, -3.02691739e-01, -4.49867767e-01,  1.26539169e+00,\n",
            "       -4.31859040e-02, -1.33766120e+00,  4.52117886e-01,  4.91286586e-01,\n",
            "        6.37298318e-01, -5.02924609e-01,  3.51655604e-01,  3.35289811e-01,\n",
            "        1.87365034e-02, -3.77441737e-02, -3.42268767e-01,  1.28392520e-01,\n",
            "        5.20983399e-01, -5.39986047e-01,  7.20227900e-01, -3.77376608e-02,\n",
            "        4.75356755e-02,  4.92000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.8077e+04, -1.4827e+00,  1.3968e+00,  1.7984e+00,  1.2494e+00,\n",
            "         -3.2699e-01,  2.6322e-01, -2.3732e-02,  5.4895e-01, -2.5039e-01,\n",
            "         -1.6531e-01, -5.7694e-01,  5.5528e-01,  3.8234e-01, -3.4100e-02,\n",
            "          4.9790e-01, -8.4777e-01,  6.8857e-01, -7.6116e-01,  8.1906e-01,\n",
            "         -6.1609e-03, -2.2608e-01, -5.4566e-01,  4.2415e-02,  6.0796e-02,\n",
            "          1.7294e-01, -4.2851e-01, -1.4709e-01,  9.7459e-02,  1.0000e+00]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.8079e+04, -4.3314e-01,  4.2117e-01,  1.9030e+00, -5.4600e-01,\n",
            "         -6.7936e-01, -5.5985e-01,  3.4448e-01,  1.5087e-01, -3.0269e-01,\n",
            "         -4.4987e-01,  1.2654e+00, -4.3186e-02, -1.3377e+00,  4.5212e-01,\n",
            "          4.9129e-01,  6.3730e-01, -5.0292e-01,  3.5166e-01,  3.3529e-01,\n",
            "          1.8737e-02, -3.7744e-02, -3.4227e-01,  1.2839e-01,  5.2098e-01,\n",
            "         -5.3999e-01,  7.2023e-01, -3.7738e-02,  4.7536e-02,  4.9200e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(1.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74304\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74305\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74306\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74307\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74308\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74309\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74310\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74311\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74312\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74313\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74314\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74315\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74316\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74317\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74318\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74319\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74320\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74321\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74322\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74323\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74324\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74325\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74326\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74327\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74328\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74329\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74330\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74331\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74332\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74333\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74334\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74335\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74336\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74337\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74338\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74339\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74340\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74341\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74342\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74343\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74344\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74345\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74346\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74347\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74348\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74349\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74350\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74351\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74352\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74353\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74354\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74355\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74356\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74357\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74358\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74359\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74360\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74361\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74362\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74363\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74364\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74365\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74366\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74367\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 3.38460000e+04,  1.41794285e+00, -3.58919861e-01, -3.48246185e-01,\n",
            "       -9.85882394e-01, -1.34986144e-01, -2.47798745e-01, -1.37860609e-01,\n",
            "       -1.67127535e-01, -1.44162428e+00,  8.08458208e-01,  4.33225205e-01,\n",
            "        7.67461085e-01,  1.36110085e+00,  2.18720414e-01, -1.50686295e-02,\n",
            "       -8.91537814e-01, -6.73390886e-01,  1.10040301e+00,  2.36620720e-01,\n",
            "       -3.36716451e-01, -8.09986175e-01, -1.94247844e+00,  1.08059289e-01,\n",
            "       -8.34628620e-01,  1.40574414e-01,  6.73703079e-01, -7.76668388e-02,\n",
            "       -8.45157328e-03,  2.49900000e+01]), action=[0], reward=0, next_state=array([ 3.38470000e+04,  1.19320933e+00, -2.57742600e-01,  1.65187168e-01,\n",
            "        5.91566330e-01, -1.96598797e-01,  3.04225289e-01, -2.80013906e-01,\n",
            "        1.44753895e-01,  8.62925017e-01, -2.47960347e-01, -1.50464687e+00,\n",
            "       -3.49498092e-01, -1.11576209e+00,  2.21459832e-02,  5.44616885e-02,\n",
            "       -3.48966088e-01,  1.45522299e-01, -6.13989599e-01,  1.77363006e-01,\n",
            "       -1.44714304e-01, -1.17578904e-01, -1.69389492e-01, -1.67410104e-01,\n",
            "       -7.30270849e-01,  5.52061430e-01,  6.26945923e-01, -2.12814667e-02,\n",
            "       -6.21909244e-04,  3.33500000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 3.3846e+04,  1.4179e+00, -3.5892e-01, -3.4825e-01, -9.8588e-01,\n",
            "         -1.3499e-01, -2.4780e-01, -1.3786e-01, -1.6713e-01, -1.4416e+00,\n",
            "          8.0846e-01,  4.3323e-01,  7.6746e-01,  1.3611e+00,  2.1872e-01,\n",
            "         -1.5069e-02, -8.9154e-01, -6.7339e-01,  1.1004e+00,  2.3662e-01,\n",
            "         -3.3672e-01, -8.0999e-01, -1.9425e+00,  1.0806e-01, -8.3463e-01,\n",
            "          1.4057e-01,  6.7370e-01, -7.7667e-02, -8.4516e-03,  2.4990e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[ 3.3847e+04,  1.1932e+00, -2.5774e-01,  1.6519e-01,  5.9157e-01,\n",
            "         -1.9660e-01,  3.0423e-01, -2.8001e-01,  1.4475e-01,  8.6293e-01,\n",
            "         -2.4796e-01, -1.5046e+00, -3.4950e-01, -1.1158e+00,  2.2146e-02,\n",
            "          5.4462e-02, -3.4897e-01,  1.4552e-01, -6.1399e-01,  1.7736e-01,\n",
            "         -1.4471e-01, -1.1758e-01, -1.6939e-01, -1.6741e-01, -7.3027e-01,\n",
            "          5.5206e-01,  6.2695e-01, -2.1281e-02, -6.2191e-04,  3.3350e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.0000]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(1.0000, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74368\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74369\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74370\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74371\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74372\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74373\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74374\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74375\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74376\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74377\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74378\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74379\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74380\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74381\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74382\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74383\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74384\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74385\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74386\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74387\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74388\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74389\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74390\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74391\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74392\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74393\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74394\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74395\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74396\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74397\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74398\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74399\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74400\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74401\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74402\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74403\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74404\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74405\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74406\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74407\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74408\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74409\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74410\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74411\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74412\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74413\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74414\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74415\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74416\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74417\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74418\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74419\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74420\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74421\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74422\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74423\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74424\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74425\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74426\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74427\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74428\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74429\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74430\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74431\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "Experiences :  [Experience(state=array([ 4.00490000e+04,  1.11014647e+00,  3.92884752e-01,  4.87900768e-01,\n",
            "        2.33158543e+00, -5.94642907e-02, -8.16178241e-02,  5.40415300e-02,\n",
            "        9.10615497e-02, -8.49412167e-01,  8.65184420e-01,  7.04402046e-01,\n",
            "       -5.44674795e-02, -1.08570597e+00,  7.12921720e-01, -1.01697393e-01,\n",
            "        1.02146649e+00, -9.10715714e-01,  2.73752467e-01, -4.98357941e-01,\n",
            "       -1.38929616e-01, -1.57632702e-01, -6.72057301e-01,  1.97795835e-02,\n",
            "       -7.56614357e-02,  3.18100807e-01, -1.83570559e-01, -2.94410706e-02,\n",
            "        1.45859013e-02,  3.50700000e+01]), action=tensor([0]), reward=1, next_state=array([ 4.00490000e+04,  9.97535639e-01, -4.83348694e-01,  6.39525344e-01,\n",
            "        7.86566273e-02, -5.20133000e-01,  5.42608798e-01, -5.14634380e-01,\n",
            "        3.19370077e-01,  3.74146418e-01, -1.88294602e-01,  1.62245255e+00,\n",
            "        1.16756020e+00, -1.07329046e-01,  1.32573326e-01,  2.88977325e-01,\n",
            "       -2.53157571e-01,  1.81148158e-01, -8.37581171e-01, -3.33929652e-01,\n",
            "        7.61876967e-05,  7.43579813e-02,  2.37735422e-01, -2.66321600e-03,\n",
            "       -1.94617947e-01,  6.30254777e-02,  1.04875540e+00, -4.06846137e-02,\n",
            "        2.52725031e-03,  7.00000000e+01]), done=False)]\n",
            "Experience sampled from memory :  (tensor([[ 4.0049e+04,  1.1101e+00,  3.9288e-01,  4.8790e-01,  2.3316e+00,\n",
            "         -5.9464e-02, -8.1618e-02,  5.4042e-02,  9.1062e-02, -8.4941e-01,\n",
            "          8.6518e-01,  7.0440e-01, -5.4467e-02, -1.0857e+00,  7.1292e-01,\n",
            "         -1.0170e-01,  1.0215e+00, -9.1072e-01,  2.7375e-01, -4.9836e-01,\n",
            "         -1.3893e-01, -1.5763e-01, -6.7206e-01,  1.9780e-02, -7.5661e-02,\n",
            "          3.1810e-01, -1.8357e-01, -2.9441e-02,  1.4586e-02,  3.5070e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64), tensor([[1.]], device='cuda:0', dtype=torch.float64), tensor([[ 4.0049e+04,  9.9754e-01, -4.8335e-01,  6.3953e-01,  7.8657e-02,\n",
            "         -5.2013e-01,  5.4261e-01, -5.1463e-01,  3.1937e-01,  3.7415e-01,\n",
            "         -1.8829e-01,  1.6225e+00,  1.1676e+00, -1.0733e-01,  1.3257e-01,\n",
            "          2.8898e-01, -2.5316e-01,  1.8115e-01, -8.3758e-01, -3.3393e-01,\n",
            "          7.6188e-05,  7.4358e-02,  2.3774e-01, -2.6632e-03, -1.9462e-01,\n",
            "          6.3025e-02,  1.0488e+00, -4.0685e-02,  2.5273e-03,  7.0000e+01]],\n",
            "       device='cuda:0', dtype=torch.float64), tensor([[0.]], device='cuda:0', dtype=torch.float64))\n",
            "Started learning\n",
            "labels_next tensor([[1.]], device='cuda:0', dtype=torch.float64)\n",
            "===========================Training loss ============================\n",
            "tensor(-99., device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<BinaryCrossEntropyBackward>)\n",
            "state_idx :  74432\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74433\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74434\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74435\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74436\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74437\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74438\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74439\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74440\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74441\n",
            "Chosing  random actions for the batch :  [1]\n",
            "state_idx :  74442\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74443\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74444\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74445\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74446\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74447\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74448\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74449\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74450\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74451\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74452\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74453\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74454\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74455\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74456\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74457\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74458\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74459\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74460\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74461\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74462\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74463\n",
            "Chosing  random actions for the batch :  [0]\n",
            "state_idx :  74464\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74465\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74466\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74467\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74468\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74469\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74470\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74471\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74472\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74473\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74474\n",
            "Predicted action based on QNetwork :  tensor([0], device='cuda:0')\n",
            "state_idx :  74475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLjulLsKqjep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}