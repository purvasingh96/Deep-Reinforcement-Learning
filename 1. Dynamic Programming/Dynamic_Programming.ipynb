{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic Programming.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/oQJ1GCenWoYQZU7592/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Deep-Reinforcement-Learning/blob/master/1.%20Dynamic%20Programming/Dynamic_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjIBpoGWWgwt",
        "colab_type": "text"
      },
      "source": [
        "### Explore FrozenLakeEnv\n",
        "\n",
        "Use the code cell below to create an instance of the [FrozenLake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RneQvbfvHpfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q matplotlib==2.2.2\n",
        "from frozen_lake import FrozenLakeEnv\n",
        "\n",
        "env = FrozenLakeEnv(is_slippery=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X0VV0BaWmyY",
        "colab_type": "text"
      },
      "source": [
        "The agent moves through a $4 \\times 4$ gridworld, with states numbered as follows:\n",
        "```\n",
        "[[ 0  1  2  3]\n",
        " [ 4  5  6  7]\n",
        " [ 8  9 10 11]\n",
        " [12 13 14 15]]\n",
        "```\n",
        "and the agent has 4 potential actions:\n",
        "```\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "```\n",
        "\n",
        "Thus, $\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$, and $\\mathcal{A} = \\{0, 1, 2, 3\\}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRDLWGfEVRMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "acfef377-82df-4c91-85f2-179f9cd5e115"
      },
      "source": [
        "print('Observation Space Dimension : ', env.observation_space)\n",
        "\n",
        "# actions : left (0), down (1), right (2), up (3)\n",
        "print('Action Space : ', env.action_space)\n",
        "\n",
        "print('Total States : ', env.nS)\n",
        "print('Total Actions : ', env.nA)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space Dimension :  Discrete(16)\n",
            "Action Space :  Discrete(4)\n",
            "Total States :  16\n",
            "Total Actions :  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFJqwqDZWu9M",
        "colab_type": "text"
      },
      "source": [
        "Dynamic programming assumes that the agent has full knowledge of the MDP.  We have already amended the `frozenlake.py` file to make the one-step dynamics accessible to the agent.  \n",
        "\n",
        " In particular, `env.P[1][0]` returns the the probability of each possible reward and next state, if the agent is in state 1 of the gridworld and decides to go left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhr4xAu1Veyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2aea500d-056c-410d-9485-f801c9fc46b2"
      },
      "source": [
        "# env.P[1][0] returns the the probability of each possible reward\n",
        "# and next state, if the agent is in state 1 of the gridworld \n",
        "# and decides to go left.\n",
        "\n",
        "print(env.P[1][0])\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCBWvsE2WzxY",
        "colab_type": "text"
      },
      "source": [
        "Each entry takes the form \n",
        "```\n",
        "prob, next_state, reward, done\n",
        "```\n",
        "where: \n",
        "- `prob` details the conditional probability of the corresponding (`next_state`, `reward`) pair, and\n",
        "- `done` is `True` if the `next_state` is a terminal state, and otherwise `False`.\n",
        "\n",
        "Thus, we can interpret `env.P[1][0]` as follows:\n",
        "$$\n",
        "\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases}\n",
        "               \\frac{1}{3} \\text{ if } s'=1, r=0\\\\\n",
        "               \\frac{1}{3} \\text{ if } s'=0, r=0\\\\\n",
        "               \\frac{1}{3} \\text{ if } s'=5, r=0\\\\\n",
        "               0 \\text{ else}\n",
        "            \\end{cases}\n",
        "$$\n",
        "\n",
        "To understand the value of `env.P[1][0]`, note that when you create a FrozenLake environment, it takes as an (optional) argument `is_slippery`, which defaults to `True`.  \n",
        "\n",
        "To see this, change the first line in the notebook from `env = FrozenLakeEnv()` to `env = FrozenLakeEnv(is_slippery=False)`.  Then, when you check `env.P[1][0]`, it should look like what you expect (i.e., `env.P[1][0] = [(1.0, 0, 0.0, False)]`).\n",
        "\n",
        "The default value for the `is_slippery` argument is `True`, and so `env = FrozenLakeEnv()` is equivalent to `env = FrozenLakeEnv(is_slippery=True)`.  In the event that `is_slippery=True`, you see that this can result in the agent moving in a direction that it did not intend (where the idea is that the ground is *slippery*, and so the agent can slide to a location other than the one it wanted).\n",
        "\n",
        "Feel free to change the code cell above to explore how the environment behaves in response to other (state, action) pairs.  \n",
        "\n",
        "Before proceeding to the next part, make sure that you set `is_slippery=True`, so that your implementations below will work with the slippery environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4jbBLOtXRAr",
        "colab_type": "text"
      },
      "source": [
        "### Part 1: Iterative Policy Evaluation\n",
        "\n",
        "In this section, you will write your own implementation of iterative policy evaluation.\n",
        "\n",
        "Your algorithm should accept four arguments as **input**:\n",
        "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
        "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
        "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
        "- `theta`: This is a very small positive number that is used to decide if the estimate has sufficiently converged to the true value function (default value: `1e-8`).\n",
        "\n",
        "The algorithm returns as **output**:\n",
        "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s` under the input policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnUrs-qPWQex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}